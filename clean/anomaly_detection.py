from typing import Tuple
import warnings

warnings.filterwarnings('ignore')

from sklearn.cluster import KMeans
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from cleanlab.classification import CleanLearning
from sklearn.dummy import DummyClassifier
import numpy as np
import pandas as pd
from tqdm import tqdm


# CleanLab å¼‚å¸¸æ£€æµ‹
def run_enhanced_cleanlab(y_true: np.ndarray, pred_probs: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """ä½¿ç”¨CleanLabè¿›è¡Œæ ‡ç­¾å¼‚å¸¸æ£€æµ‹"""
    print("ğŸ§¹ è¿è¡Œ CleanLab...")

    try:
        # ç¡®ä¿è¾“å…¥æ•°æ®çš„æœ‰æ•ˆæ€§
        if len(y_true) != len(pred_probs):
            raise ValueError(f"æ ‡ç­¾æ•°é‡ ({len(y_true)}) ä¸é¢„æµ‹æ¦‚ç‡æ•°é‡ ({len(pred_probs)}) ä¸åŒ¹é…")

        if pred_probs.shape[1] != len(np.unique(y_true)):
            print(f"âš ï¸ è­¦å‘Š: é¢„æµ‹æ¦‚ç‡ç»´åº¦ ({pred_probs.shape[1]}) ä¸ç±»åˆ«æ•°é‡ ({len(np.unique(y_true))}) ä¸åŒ¹é…")

        # åˆ›å»ºCleanLearningå®ä¾‹
        cl = CleanLearning(
            DummyClassifier(strategy="prior"),
            seed=42,
            cv_n_folds=1
        )

        # æŸ¥æ‰¾æ ‡ç­¾é—®é¢˜
        issues = cl.find_label_issues(
            labels=y_true,
            pred_probs=pred_probs,
        )

        # è·å–å¯ç–‘æ ·æœ¬ç´¢å¼•å’Œåˆ†æ•°
        suspect_indices = issues.query("is_label_issue == True").index.to_numpy()

        # è·å–è´¨é‡åˆ†æ•°ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
        if "label_quality" in issues.columns:
            quality_scores = issues["label_quality"].to_numpy()
        else:
            # ä½¿ç”¨é¢„æµ‹æ¦‚ç‡çš„ç½®ä¿¡åº¦ä½œä¸ºè´¨é‡åˆ†æ•°
            quality_scores = pred_probs.max(axis=1)

        print(f"ğŸ” CleanLabæ£€æµ‹åˆ° {len(suspect_indices)} ä¸ªå¯ç–‘æ ·æœ¬")
        return suspect_indices, quality_scores

    except Exception as e:
        print(f"âŒ CleanLabæ£€æµ‹å¤±è´¥: {e}")
        # è¿”å›ç©ºç»“æœ
        return np.array([]), np.ones(len(y_true))


# K-Means ç±»å†…å¼‚å¸¸æ£€æµ‹
def run_kmeans_detection(y_true: np.ndarray, pred_probs: np.ndarray, config: dict) -> Tuple[np.ndarray, np.ndarray]:
    """ä½¿ç”¨K-Meansè¿›è¡Œç±»å†…å¼‚å¸¸æ£€æµ‹"""
    print("ğŸ¯ è¿è¡Œ K-Means ç±»å†…å¼‚å¸¸æ£€æµ‹...")

    try:
        suspect_indices = []
        anomaly_scores = np.ones(len(y_true))  # åˆ†æ•°,é»˜è®¤åˆ†æ•°ä¸º1(æ­£å¸¸)è¶Šå°è¶Šå¯ç–‘
        unique_labels = np.unique(y_true)

        for label in tqdm(unique_labels, desc="K-Meansç±»å†…æ£€æµ‹", leave=False):
            class_mask = y_true == label
            class_indices = np.where(class_mask)[0]
            class_probs = pred_probs[class_mask]

            # æ£€æŸ¥æ ·æœ¬æ•°é‡æ˜¯å¦è¶³å¤Ÿè¿›è¡Œèšç±»
            if len(class_indices) < config["min_clusters"]:
                print(f"   è·³è¿‡ç±»åˆ« {label}: æ ·æœ¬æ•°é‡ä¸è¶³ ({len(class_indices)} < {config['min_clusters']})")
                continue

            n_samples = len(class_indices)
            n_clusters = max(
                config["min_clusters"],
                min(config["max_clusters"], int(n_samples * config["n_clusters_ratio"]))
            )

            # æ‰§è¡ŒK-Meansèšç±»
            kmeans = KMeans(
                n_clusters=n_clusters,
                random_state=config["random_state"],
                n_init=10
            )
            cluster_labels = kmeans.fit_predict(class_probs)

            # è®¡ç®—åˆ°æœ€è¿‘èšç±»ä¸­å¿ƒçš„è·ç¦»
            distances = np.min(kmeans.transform(class_probs), axis=1)

            # ç¡®å®šå¼‚å¸¸é˜ˆå€¼
            contamination_ratio = config["contamination"]
            anomaly_threshold = np.percentile(distances, (1 - contamination_ratio) * 100)

            # è¯†åˆ«å¼‚å¸¸æ ·æœ¬
            anomaly_mask = distances > anomaly_threshold
            class_anomalies = class_indices[anomaly_mask]
            suspect_indices.extend(class_anomalies)

            # è®¡ç®—å¼‚å¸¸åˆ†æ•°ï¼ˆè·ç¦»è¶Šå¤§ï¼Œåˆ†æ•°è¶Šä½ï¼‰
            max_dist = distances.max()
            if max_dist > 0:
                for i, dist in enumerate(distances):
                    anomaly_scores[class_indices[i]] = 1 - (dist / max_dist)

            print(f"   ç±»åˆ« {label}: {len(class_anomalies)}/{len(class_indices)} ä¸ªå¼‚å¸¸æ ·æœ¬")

        suspect_indices = np.array(suspect_indices)
        print(f"ğŸ¯ K-Meansæ£€æµ‹åˆ° {len(suspect_indices)} ä¸ªç±»å†…å¼‚å¸¸æ ·æœ¬")

        return suspect_indices, anomaly_scores

    except Exception as e:
        print(f"âŒ K-Meansæ£€æµ‹å¤±è´¥: {e}")
        return np.array([]), np.ones(len(y_true))


# Isolation Forest å…¨å±€å¼‚å¸¸æ£€æµ‹
def run_isolation_forest(y_true: np.ndarray, pred_probs: np.ndarray, config: dict) -> Tuple[np.ndarray, np.ndarray]:
    """ä½¿ç”¨Isolation Forestè¿›è¡Œå…¨å±€å¼‚å¸¸æ£€æµ‹"""
    print("ğŸŒ² è¿è¡Œ Isolation Forest å…¨å±€å¼‚å¸¸æ£€æµ‹...")

    try:
        # æ„å»ºç‰¹å¾å‘é‡
        features = []

        # 1. åŸå§‹é¢„æµ‹æ¦‚ç‡
        features.append(pred_probs)

        # 2. æœ€å¤§æ¦‚ç‡ï¼ˆç½®ä¿¡åº¦ï¼‰
        confidence = pred_probs.max(axis=1).reshape(-1, 1)
        features.append(confidence)

        # 3. ç†µï¼ˆä¸ç¡®å®šæ€§è¶Šé«˜è¶Šä¸æ­£ç¡®ï¼‰
        entropy = -np.sum(pred_probs * np.log(pred_probs + 1e-8), axis=1).reshape(-1, 1)
        features.append(entropy)

        # 4. æ ‡ç­¾ä¸€è‡´æ€§ï¼ˆçœŸå®æ ‡ç­¾å¯¹åº”çš„é¢„æµ‹æ¦‚ç‡ï¼‰
        label_consistency = pred_probs[np.arange(len(y_true)), y_true].reshape(-1, 1)
        features.append(label_consistency)

        # 5. é¢„æµ‹æ ‡ç­¾ä¸çœŸå®æ ‡ç­¾çš„å·®å¼‚
        pred_labels = pred_probs.argmax(axis=1)
        label_mismatch = (pred_labels != y_true).astype(float).reshape(-1, 1)
        features.append(label_mismatch)

        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        X = np.hstack(features)

        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # å¦‚æœç‰¹å¾ç»´åº¦è¿‡é«˜ï¼Œä½¿ç”¨PCAé™ç»´
        if X_scaled.shape[1] > 15:
            pca = PCA(n_components=min(15, X_scaled.shape[1]))
            X_scaled = pca.fit_transform(X_scaled)
            print(f"   ä½¿ç”¨PCAé™ç»´: {X.shape[1]} -> {X_scaled.shape[1]}")

        # åˆ›å»ºIsolation Forestæ¨¡å‹
        iso_forest = IsolationForest(
            contamination=config["contamination"],
            n_estimators=config["n_estimators"],
            random_state=config["random_state"],
            n_jobs=config["n_jobs"]
        )

        # è®­ç»ƒå¹¶é¢„æµ‹
        anomaly_labels = iso_forest.fit_predict(X_scaled)
        anomaly_scores = iso_forest.score_samples(X_scaled)

        # å°†å¼‚å¸¸åˆ†æ•°æ ‡å‡†åŒ–åˆ°[0,1]èŒƒå›´ï¼Œåˆ†æ•°è¶Šé«˜è¶Šæ­£å¸¸
        anomaly_scores = (anomaly_scores - anomaly_scores.min()) / (anomaly_scores.max() - anomaly_scores.min())

        # è·å–å¼‚å¸¸æ ·æœ¬ç´¢å¼•
        suspect_indices = np.where(anomaly_labels == -1)[0]
        print(f"ğŸŒ² Isolation Forestæ£€æµ‹åˆ° {len(suspect_indices)} ä¸ªå…¨å±€å¼‚å¸¸æ ·æœ¬")

        return suspect_indices, anomaly_scores

    except Exception as e:
        print(f"âŒ Isolation Forestæ£€æµ‹å¤±è´¥: {e}")
        return np.array([]), np.ones(len(y_true))


# é›†æˆå¤šä¸ªç®—æ³•çš„å¼‚å¸¸æ£€æµ‹ç»“æœ
def ensemble_decision(
        y_true: np.ndarray,
        cleanlab_suspects: np.ndarray,
        cleanlab_scores: np.ndarray,
        kmeans_suspects: np.ndarray,
        kmeans_scores: np.ndarray,
        isolation_suspects: np.ndarray,
        isolation_scores: np.ndarray,
        config: dict
) -> pd.DataFrame:
    print("ğŸ¤ é›†æˆå¤šç®—æ³•æ£€æµ‹ç»“æœ...")

    try:
        n_samples = len(y_true)
        results = []

        # åˆ›å»ºæŠ•ç¥¨çŸ©é˜µ
        votes = np.zeros((n_samples, 3), dtype=int)
        votes[cleanlab_suspects, 0] = 1
        votes[kmeans_suspects, 1] = 1
        votes[isolation_suspects, 2] = 1

        # ç®—æ³•æƒé‡
        weights = np.array([
            config["quality_score_weight"]["cleanlab"],
            config["quality_score_weight"]["kmeans"],
            config["quality_score_weight"]["isolation"]
        ])

        # å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œé›†æˆå†³ç­–
        for i in range(n_samples):
            vote_count = votes[i].sum()
            weighted_vote = np.dot(votes[i], weights)

            # è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°
            composite_score = (
                    cleanlab_scores[i] * config["quality_score_weight"]["cleanlab"] +
                    kmeans_scores[i] * config["quality_score_weight"]["kmeans"] +
                    isolation_scores[i] * config["quality_score_weight"]["isolation"]
            )

            # ç®€åŒ–çš„åˆ¤æ–­é€»è¾‘ï¼šæŠ•ç¥¨ OR ä½åˆ†æ•°
            is_suspect = (
                    vote_count >= config["voting_threshold"] or  # æŠ•ç¥¨é˜ˆå€¼
                    composite_score <= config["score_threshold"]  # åˆ†æ•°é˜ˆå€¼
            )

            results.append({
                "index": i,
                "true_label": y_true[i],
                "cleanlab_suspect": i in cleanlab_suspects,
                "kmeans_suspect": i in kmeans_suspects,
                "isolation_suspect": i in isolation_suspects,
                "vote_count": int(vote_count),
                "weighted_vote": weighted_vote,
                "cleanlab_score": cleanlab_scores[i],
                "kmeans_score": kmeans_scores[i],
                "isolation_score": isolation_scores[i],
                "composite_score": composite_score,
                "is_suspect": is_suspect
            })

        # åˆ›å»ºç»“æœDataFrame
        df_results = pd.DataFrame(results)
        total_suspects = df_results["is_suspect"].sum()

        print(f"ğŸ¤ é›†æˆç»“æœ: {total_suspects}/{n_samples} æ ·æœ¬è¢«æ ‡è®°ä¸ºå¯ç–‘ ({total_suspects / n_samples * 100:.1f}%)")

        print("ğŸ“Š å„ç®—æ³•æ£€æµ‹ç»Ÿè®¡:")
        print(f"   CleanLab: {len(cleanlab_suspects)} ä¸ª ({len(cleanlab_suspects) / n_samples * 100:.1f}%)")
        print(f"   K-Means: {len(kmeans_suspects)} ä¸ª ({len(kmeans_suspects) / n_samples * 100:.1f}%)")
        print(f"   Isolation Forest: {len(isolation_suspects)} ä¸ª ({len(isolation_suspects) / n_samples * 100:.1f}%)")
        print(f"   æœ€ç»ˆé›†æˆ: {total_suspects} ä¸ª ({total_suspects / n_samples * 100:.1f}%)")

        # åˆ†æè§¦å‘åŸå› 
        vote_triggered = df_results[df_results["vote_count"] >= config["voting_threshold"]].shape[0]
        score_triggered = df_results[df_results["composite_score"] <= config["score_threshold"]].shape[0]
        both_triggered = df_results[
            (df_results["vote_count"] >= config["voting_threshold"]) &
            (df_results["composite_score"] <= config["score_threshold"])
            ].shape[0]

        print(f"\nğŸ¯ è§¦å‘åŸå› åˆ†æ:")
        print(f"   æŠ•ç¥¨è§¦å‘: {vote_triggered} ä¸ª")
        print(f"   åˆ†æ•°è§¦å‘: {score_triggered} ä¸ª")
        print(f"   åŒé‡è§¦å‘: {both_triggered} ä¸ª")

        # æŒ‰ç»¼åˆåˆ†æ•°æ’åºï¼Œåˆ†æ•°è¶Šä½è¶Šå¯ç–‘
        return df_results.sort_values("composite_score", ascending=True)

    except Exception as e:
        print(f"âŒ é›†æˆå†³ç­–å¤±è´¥: {e}")
        # è¿”å›åŸºæœ¬çš„DataFrame
        return pd.DataFrame({
            "index": range(len(y_true)),
            "true_label": y_true,
            "is_suspect": [False] * len(y_true),
            "composite_score": [1.0] * len(y_true)
        })