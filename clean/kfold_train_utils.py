from pathlib import Path
from typing import List, Dict, Tuple
import numpy as np
import pandas as pd
import torch
from sklearn.model_selection import StratifiedKFold
from tqdm import tqdm
from ultralytics import YOLO
import yaml
import time
import hashlib
import json
import shutil
import os


def create_yolo_dataset_structure(
        fold_idx: int,
        train_idx: List[int],
        val_idx: List[int],
        img_paths: List[Path],
        labels: List[int],
        label_map: Dict[int, str],
        fold_dir: Path
) -> Path:
    """ä¸ºYOLOåˆ†ç±»åˆ›å»ºæ ‡å‡†æ•°æ®é›†ç»“æ„"""

    # åˆ›å»ºYOLOåˆ†ç±»æ•°æ®é›†ç»“æ„
    dataset_dir = fold_dir / "dataset"
    train_dir = dataset_dir / "train"
    val_dir = dataset_dir / "val"

    # æ¸…ç†å¹¶åˆ›å»ºç›®å½•
    if dataset_dir.exists():
        shutil.rmtree(dataset_dir)

    # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºç›®å½•
    for label_id, label_name in label_map.items():
        (train_dir / label_name).mkdir(parents=True, exist_ok=True)
        (val_dir / label_name).mkdir(parents=True, exist_ok=True)

    # å¤åˆ¶è®­ç»ƒé›†å›¾ç‰‡
    train_paths = [img_paths[i] for i in train_idx]
    train_labels = [labels[i] for i in train_idx]

    for img_path, label in zip(train_paths, train_labels):
        label_name = label_map[label]
        dst_path = train_dir / label_name / img_path.name
        shutil.copy2(img_path, dst_path)

    # å¤åˆ¶éªŒè¯é›†å›¾ç‰‡
    val_paths = [img_paths[i] for i in val_idx]
    val_labels = [labels[i] for i in val_idx]

    for img_path, label in zip(val_paths, val_labels):
        label_name = label_map[label]
        dst_path = val_dir / label_name / img_path.name
        shutil.copy2(img_path, dst_path)

    print(f"      ğŸ“ åˆ›å»ºæ•°æ®é›†ç»“æ„: è®­ç»ƒé›† {len(train_idx)} å¼ , éªŒè¯é›† {len(val_idx)} å¼ ")

    return dataset_dir


def save_and_train_one_fold(
        fold_idx: int,
        train_idx: List[int],
        val_idx: List[int],
        img_paths: List[Path],
        labels: List[int],
        label_map: Dict[int, str],
        params: Dict,
        dataset_name: str,
        use_cache: bool = True
) -> Path:
    """è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹æƒé‡ï¼Œæ”¯æŒç¼“å­˜åŠŸèƒ½"""
    print(f"      â³ è®­ç»ƒ Fold {fold_idx}...")

    # ç®€åŒ–çš„ç¼“å­˜è·¯å¾„ï¼šweights/dataset_name/fold_idx/
    cache_base_dir = Path("weights") / dataset_name
    fold_dir = cache_base_dir / f"fold_{fold_idx}"
    fold_dir.mkdir(parents=True, exist_ok=True)

    # æœ€ä½³æ¨¡å‹æƒé‡è·¯å¾„
    best_ckpt = fold_dir / "train" / "weights" / "best.pt"

    # ç®€åŒ–ç¼“å­˜æ£€æŸ¥ï¼šåªæ£€æŸ¥æƒé‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if use_cache and best_ckpt.exists():
        print(f"      âœ… ä½¿ç”¨ç¼“å­˜çš„ Fold {fold_idx} æ¨¡å‹")
        return best_ckpt

    # åˆ›å»ºYOLOæ•°æ®é›†ç»“æ„
    dataset_dir = create_yolo_dataset_structure(
        fold_idx, train_idx, val_idx, img_paths, labels, label_map, fold_dir
    )

    # YOLOæ¨¡å‹è®­ç»ƒ
    print(f"      ğŸ”„ å¼€å§‹è®­ç»ƒ Fold {fold_idx}...")
    base_ckpt = "yolo11s-cls.pt"
    model = YOLO(base_ckpt)

    model.train(
        data=str(dataset_dir),  # ä¼ å…¥æ•°æ®é›†ç›®å½•è·¯å¾„
        epochs=params["epochs"],
        imgsz=params["imgsz"],
        batch=params["batch"],
        patience=params["patience"],
        project=str(fold_dir),
        cache=params["cache"],
        amp=params["amp"],
        workers=params["workers"],
        name="",
        exist_ok=True,
        device=params["device"],
    )

    print(f"      âœ… Fold {fold_idx} è®­ç»ƒå®Œæˆ")
    return best_ckpt


def generate_dataset_name(img_paths: List[Path], n_folds: int, data_dir: Path = None) -> str:
    """ç”Ÿæˆç¨³å®šçš„æ•°æ®é›†åç§°ï¼ˆç¡®ä¿ç›¸åŒæ•°æ®é›†æ¯æ¬¡ç”Ÿæˆç›¸åŒåç§°ï¼‰"""
    if data_dir:
        # ä½¿ç”¨æ•°æ®é›†ç›®å½•åç§°å’Œæ–‡ä»¶æ•°é‡
        dataset_name = data_dir.name
        file_count = len(img_paths)
        return f"{dataset_name}_{file_count}_{n_folds}fold"
    else:
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„çˆ¶ç›®å½•åç§°
        if img_paths:
            parent_name = img_paths[0].parent.name
            file_count = len(img_paths)
            return f"{parent_name}_{file_count}_{n_folds}fold"
        else:
            return f"dataset_{n_folds}fold_empty"


def train_kfold_models(
        img_paths: List[Path],
        labels: List[int],
        label_map: Dict[int, str],
        n_folds: int = 5,
        params: dict = {},
        data_dir: Path = None,
        use_cache: bool = True
) -> List[Path]:
    """K-Foldè®­ç»ƒæ¨¡å‹å¹¶è¿”å›æ¯æŠ˜çš„æƒé‡è·¯å¾„ï¼Œæ”¯æŒç¼“å­˜åŠŸèƒ½"""
    print("ğŸ“¦ å¯åŠ¨ K æŠ˜è®­ç»ƒ...")

    # ç”Ÿæˆç¨³å®šçš„æ•°æ®é›†åç§°
    dataset_name = generate_dataset_name(img_paths, n_folds, data_dir)
    print(f"   æ•°æ®é›†æ ‡è¯†: {dataset_name}")

    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)
    weight_paths = []

    # å¾ªç¯æ¯ä¸€æŠ˜çš„è®­ç»ƒ
    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(img_paths, labels)):
        print(f"ğŸ“ è®­ç»ƒ Fold {fold_idx + 1}/{n_folds}:")
        weight_path = save_and_train_one_fold(
            fold_idx, train_idx, val_idx, img_paths, labels, label_map,
            params, dataset_name, use_cache
        )
        weight_paths.append(weight_path)

    print("âœ… K æŠ˜è®­ç»ƒå®Œæˆ")
    return weight_paths


def kfold_predict(
        img_paths: List[Path],
        labels: List[int],
        weight_paths: List[Path],
        n_folds: int,
        batch_size: int,
        conf_thresh: float,
        config: dict,
        data_dir: Path = None,
        use_cache: bool = True
) -> Tuple[np.ndarray, np.ndarray]:
    """ä½¿ç”¨KæŠ˜æ¨¡å‹é¢„æµ‹ï¼Œæ”¯æŒç¼“å­˜åŠŸèƒ½"""
    print("ğŸ” å¼€å§‹ K æŠ˜æ¨¡å‹é¢„æµ‹...")

    # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ•°æ®é›†åç§°ç”Ÿæˆæ–¹æ³•
    dataset_name = generate_dataset_name(img_paths, n_folds, data_dir)
    print(f"   æ•°æ®é›†æ ‡è¯†: {dataset_name}")

    cache_dir = config["CACHE_DIR"]
    cache_dir.mkdir(parents=True, exist_ok=True)
    cache_path = cache_dir / f"pred_{dataset_name}.npz"

    # å¦‚æœç¼“å­˜å·²å­˜åœ¨ä¸”å…è®¸ä½¿ç”¨ç¼“å­˜ï¼Œåˆ™åŠ è½½ç¼“å­˜
    if use_cache and cache_path.exists():
        try:
            print("ğŸ”„ ä½¿ç”¨ç¼“å­˜çš„é¢„æµ‹æ•°æ®")
            data = np.load(cache_path)
            return data["y_true"], data["y_pred"]
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥: {e}ï¼Œé‡æ–°é¢„æµ‹")

    all_probs = []  # å­˜å‚¨æ‰€æœ‰æŠ˜çš„é¢„æµ‹æ¦‚ç‡
    all_true = np.array(labels)  # å­˜å‚¨çœŸå®æ ‡ç­¾
    n_classes = len(set(labels))  # ç±»åˆ«æ•°é‡

    # å¾ªç¯éå†æ¯ä¸ªæŠ˜çš„æƒé‡æ–‡ä»¶è¿›è¡Œé¢„æµ‹
    for i, weight_path in enumerate(weight_paths):
        print(f"   ä½¿ç”¨ Fold {i + 1} æƒé‡è¿›è¡Œé¢„æµ‹: {weight_path.name}")

        if not weight_path.exists():
            raise FileNotFoundError(f"æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {weight_path}")

        model = YOLO(str(weight_path))

        # æ‰¹é‡é¢„æµ‹ï¼Œè·å–æ¯ä¸ªå›¾ç‰‡çš„é¢„æµ‹ç»“æœ
        fold_probs = []

        # åˆ†æ‰¹å¤„ç†å›¾ç‰‡
        for batch_start in tqdm(range(0, len(img_paths), batch_size),
                                desc=f"é¢„æµ‹ Fold {i + 1}", leave=False):
            batch_end = min(batch_start + batch_size, len(img_paths))
            batch_paths = [str(p) for p in img_paths[batch_start:batch_end]]

            # YOLOé¢„æµ‹
            results = model.predict(batch_paths, conf=conf_thresh, verbose=False)

            # å¤„ç†é¢„æµ‹ç»“æœ
            for result in results:
                if hasattr(result, 'probs') and result.probs is not None:
                    # åˆ†ç±»ä»»åŠ¡çš„æ¦‚ç‡
                    prob = result.probs.data.cpu().numpy()
                    if len(prob) == n_classes:
                        fold_probs.append(prob)
                    else:
                        # å¦‚æœæ¦‚ç‡ç»´åº¦ä¸åŒ¹é…ï¼Œåˆ›å»ºé»˜è®¤æ¦‚ç‡åˆ†å¸ƒ
                        default_prob = np.ones(n_classes) / n_classes
                        fold_probs.append(default_prob)
                else:
                    # å¦‚æœæ²¡æœ‰æ¦‚ç‡ä¿¡æ¯ï¼Œåˆ›å»ºé»˜è®¤æ¦‚ç‡åˆ†å¸ƒ
                    default_prob = np.ones(n_classes) / n_classes
                    fold_probs.append(default_prob)

        fold_probs = np.array(fold_probs)
        all_probs.append(fold_probs)
        print(f"   Fold {i + 1} é¢„æµ‹å®Œæˆï¼Œå½¢çŠ¶: {fold_probs.shape}")

    # æ£€æŸ¥æ‰€æœ‰foldçš„é¢„æµ‹ç»“æœå½¢çŠ¶æ˜¯å¦ä¸€è‡´
    if len(set(probs.shape for probs in all_probs)) > 1:
        print("âš ï¸ è­¦å‘Š: ä¸åŒfoldçš„é¢„æµ‹ç»“æœå½¢çŠ¶ä¸ä¸€è‡´")
        # ç»Ÿä¸€å½¢çŠ¶
        target_shape = all_probs[0].shape
        for i in range(len(all_probs)):
            if all_probs[i].shape != target_shape:
                print(f"   è°ƒæ•´ Fold {i + 1} çš„é¢„æµ‹ç»“æœå½¢çŠ¶")
                all_probs[i] = np.resize(all_probs[i], target_shape)

    # è®¡ç®—æ‰€æœ‰æŠ˜çš„å¹³å‡é¢„æµ‹æ¦‚ç‡
    all_probs = np.array(all_probs)
    pred_probs = np.mean(all_probs, axis=0)

    print(f"âœ… é›†æˆé¢„æµ‹å®Œæˆï¼Œæœ€ç»ˆå½¢çŠ¶: {pred_probs.shape}")

    # ä¿å­˜é¢„æµ‹ç»“æœåˆ°ç¼“å­˜
    if use_cache:
        try:
            print("ğŸ’¾ ä¿å­˜é¢„æµ‹ç»“æœåˆ°ç¼“å­˜")
            np.savez(cache_path, y_true=all_true, y_pred=pred_probs)
            print(f"   ç¼“å­˜ä¿å­˜è‡³: {cache_path}")
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

    print("âœ… K æŠ˜é¢„æµ‹å®Œæˆ")
    return all_true, pred_probs