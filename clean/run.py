from pathlib import Path
import time
import sys
import logging
from datetime import datetime
from kfold_train_utils import train_kfold_models, kfold_predict
from data_utils_affectnet import load_dataset, export_enhanced_results
from anomaly_detection import run_enhanced_cleanlab, run_kmeans_detection, run_isolation_forest, ensemble_decision
from anomaly_visualization import AnomalyDetectionVisualizer


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ—¥å¿—é…ç½®ç±»
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class DualOutput:
    """åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶çš„ç±»"""

    def __init__(self, log_file):
        self.terminal = sys.stdout
        self.log_file = open(log_file, 'w', encoding='utf-8')

    def write(self, message):
        self.terminal.write(message)
        self.log_file.write(message)
        self.log_file.flush()  # ç¡®ä¿ç«‹å³å†™å…¥æ–‡ä»¶

    def flush(self):
        self.terminal.flush()
        self.log_file.flush()

    def close(self):
        self.log_file.close()


def setup_logging(log_dir: Path) -> tuple:
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_dir.mkdir(parents=True, exist_ok=True)

    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ—¥å¿—æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"affectnet_cleaning_{timestamp}.log"

    # åˆ›å»ºåŒè¾“å‡ºå¯¹è±¡
    dual_output = DualOutput(log_file)

    # é‡å®šå‘stdout
    original_stdout = sys.stdout
    sys.stdout = dual_output

    print(f"ğŸ“ æ—¥å¿—ç³»ç»Ÿå·²å¯åŠ¨")
    print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)

    return dual_output, original_stdout, log_file


def cleanup_logging(dual_output, original_stdout, log_file: Path):
    """æ¸…ç†æ—¥å¿—ç³»ç»Ÿ"""
    print("=" * 70)
    print(f"â° ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“ æ—¥å¿—å·²ä¿å­˜è‡³: {log_file}")

    # æ¢å¤åŸå§‹stdout
    sys.stdout = original_stdout
    dual_output.close()

    print(f"âœ… æ—¥å¿—ç³»ç»Ÿå·²å…³é—­ï¼Œæ—¥å¿—æ–‡ä»¶: {log_file}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# é…ç½®å‚æ•° (ä¿ç•™åŸå§‹è„šæœ¬ä¸­çš„CONFIG)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SCRIPT_DIR: Path = Path(__file__).resolve().parent
PROJECT_ROOT: Path = SCRIPT_DIR.parent

CONFIG: dict = {
    "DATA_DIR": PROJECT_ROOT / "datasets/cls/processed/affectnet-1",  # æ•°æ®é›†æ ¹ç›®å½•
    "KFOLD": 5,  # KæŠ˜äº¤å‰éªŒè¯æŠ˜æ•°
    "BATCH_SIZE": 32,  # æ¨ç†æ‰¹å¤§å°
    "CONF_THRESH": 0.001,  # ç½®ä¿¡åº¦é˜ˆå€¼
    "OUTPUT_DIR": SCRIPT_DIR / "caches",  # è¾“å‡ºç›®å½•
    "SUSPECTS_DIR": SCRIPT_DIR / "caches/suspects",  # å¯ç–‘å›¾ç‰‡è¾“å‡ºç›®å½•
    "CLEAN_DIR": SCRIPT_DIR / "caches/clean",  # å¹²å‡€å›¾ç‰‡è¾“å‡ºç›®å½•
    "LOG_DIR": SCRIPT_DIR / "logs",  # æ—¥å¿—ç›®å½•
    "VISUALIZATION_DIR": SCRIPT_DIR / "visualizations",  # å¯è§†åŒ–è¾“å‡ºç›®å½•
    "SAVE_IMGS": True,  # æ˜¯å¦ä¿å­˜å›¾ç‰‡
    "CACHE_DIR": SCRIPT_DIR / "caches/cache",  # ç¼“å­˜ç›®å½•
    "ENABLE_VISUALIZATION": True,  # æ˜¯å¦å¯ç”¨å¯è§†åŒ–

    # ç¼“å­˜æ§åˆ¶
    "USE_CACHE": True,  # æ˜¯å¦ä½¿ç”¨ç¼“å­˜åŠŸèƒ½

    "TRAIN_PARAMS": {
        "epochs": 150,
        "imgsz": 224,
        "batch": 32,
        "patience": 20,
        "device": "0"
    },

    # K-Meanså‚æ•°
    "KMEANS_CONFIG": {
        "n_clusters_ratio": 0.1,  # æ¨è: 0.05-0.2
        "min_clusters": 2,  # æ¨è: 2-5
        "max_clusters": 20,  # æ¨è: 10-50
        "contamination": 0.1,  # æ¨è: 0.05-0.2
        "random_state": 42  # æ¨è: å›ºå®šå€¼ç¡®ä¿å¯é‡å¤
    },

    # Isolation Forestå‚æ•°
    "ISOLATION_CONFIG": {
        "contamination": 0.1,  # é»˜è®¤: "auto", æ¨è: 0.05-0.2
        "n_estimators": 100,  # é»˜è®¤: 100
        "random_state": 42,  # é»˜è®¤: None
        "n_jobs": -1  # é»˜è®¤: None
    },

    # é›†æˆæŠ•ç¥¨å‚æ•°
    "ENSEMBLE_CONFIG": {
        "voting_threshold": 2,  # è‡³å°‘3ä¸ªç®—æ³•åŒæ„æ‰æ ‡è®°ä¸ºå¯ç–‘(1-3)
        "score_threshold": 0.3,  # ç»¼åˆåˆ†æ•°ä½äº0.1ä¹Ÿæ ‡è®°ä¸ºå¯ç–‘

        # ç®—æ³•æƒé‡
        "quality_score_weight": {
            "cleanlab": 0.5,  # CleanLabé€šå¸¸æœ€å¯é 
            "kmeans": 0.1,  # K-Meansé€‚åˆç±»å†…å¼‚å¸¸,ä¸é€‚åˆè®¡ç®—åˆ†æ•°
            "isolation": 0.4  # Isolation Forestæ‰¾å…¨å±€å¼‚å¸¸
        }
    }
}


def print_config_summary():
    """æ‰“å°é…ç½®æ‘˜è¦"""
    print("âš™ï¸ é…ç½®æ‘˜è¦:")
    print(f"   æ•°æ®ç›®å½•: {CONFIG['DATA_DIR']}")
    print(f"   KæŠ˜æ•°: {CONFIG['KFOLD']}")
    print(f"   æ‰¹å¤§å°: {CONFIG['BATCH_SIZE']}")
    print(f"   ç¼“å­˜åŠŸèƒ½: {'å¯ç”¨' if CONFIG['USE_CACHE'] else 'ç¦ç”¨'}")
    print(f"   ä¿å­˜å›¾ç‰‡: {'æ˜¯' if CONFIG['SAVE_IMGS'] else 'å¦'}")
    print(f"   å¯è§†åŒ–: {'å¯ç”¨' if CONFIG['ENABLE_VISUALIZATION'] else 'ç¦ç”¨'}")
    print(f"   å¯è§†åŒ–ç›®å½•: {CONFIG['VISUALIZATION_DIR']}")
    print(f"   è®­ç»ƒè½®æ•°: {CONFIG['TRAIN_PARAMS']['epochs']}")
    print(f"   å›¾ç‰‡å°ºå¯¸: {CONFIG['TRAIN_PARAMS']['imgsz']}")
    print(f"   è®¾å¤‡: {CONFIG['TRAIN_PARAMS']['device']}")
    print("-" * 50)


def main():
    # è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
    dual_output, original_stdout, log_file = setup_logging(CONFIG["LOG_DIR"])

    start_time = time.time()

    try:
        print("ğŸ¯ å¼€å§‹å¢å¼ºç‰ˆAffectNetæ ‡ç­¾æ¸…æ´—")
        print_config_summary()

        # 1. åŠ è½½æ•°æ®é›†
        print("\nğŸ“Š æ­¥éª¤ 1/9: åŠ è½½æ•°æ®é›†")
        img_paths, labels, label_map = load_dataset(CONFIG["DATA_DIR"])
        print(f"   åŠ è½½å®Œæˆ: {len(img_paths)} å¼ å›¾ç‰‡, {len(label_map)} ä¸ªç±»åˆ«")

        # 2. KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒ
        print("\nğŸš€ æ­¥éª¤ 2/9: KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒ")
        weight_paths = train_kfold_models(
            img_paths,
            labels,
            label_map,
            CONFIG["KFOLD"],
            CONFIG["TRAIN_PARAMS"],
            data_dir=CONFIG["DATA_DIR"],
            use_cache=CONFIG["USE_CACHE"]
        )
        print(f"   è®­ç»ƒå®Œæˆ: {len(weight_paths)} ä¸ªæ¨¡å‹æƒé‡")

        # 3. KæŠ˜äº¤å‰éªŒè¯æ¨ç†
        print("\nğŸ” æ­¥éª¤ 3/9: KæŠ˜äº¤å‰éªŒè¯æ¨ç†")
        y_true, pred_probs = kfold_predict(
            img_paths,
            labels,
            weight_paths,
            CONFIG["KFOLD"],
            CONFIG["BATCH_SIZE"],
            CONFIG["CONF_THRESH"],
            CONFIG,
            data_dir=CONFIG["DATA_DIR"],
            use_cache=CONFIG["USE_CACHE"]
        )
        print(f"   æ¨ç†å®Œæˆ: é¢„æµ‹å½¢çŠ¶ {pred_probs.shape}")

        # 4. CleanLabå¼‚å¸¸æ£€æµ‹
        print("\nğŸ§¹ æ­¥éª¤ 4/9: CleanLabå¼‚å¸¸æ£€æµ‹")
        cleanlab_suspects, cleanlab_scores = run_enhanced_cleanlab(y_true, pred_probs)
        print(f"   æ£€æµ‹å®Œæˆ: {len(cleanlab_suspects)} ä¸ªå¯ç–‘æ ·æœ¬")

        # 5. K-Meansç±»å†…å¼‚å¸¸æ£€æµ‹
        print("\nğŸ¯ æ­¥éª¤ 5/9: K-Meansç±»å†…å¼‚å¸¸æ£€æµ‹")
        kmeans_suspects, kmeans_scores = run_kmeans_detection(y_true, pred_probs, CONFIG["KMEANS_CONFIG"])
        print(f"   æ£€æµ‹å®Œæˆ: {len(kmeans_suspects)} ä¸ªå¯ç–‘æ ·æœ¬")

        # 6. Isolation Forestå…¨å±€æ£€æµ‹
        print("\nğŸŒ² æ­¥éª¤ 6/9: Isolation Forestå…¨å±€æ£€æµ‹")
        isolation_suspects, isolation_scores = run_isolation_forest(y_true, pred_probs, CONFIG["ISOLATION_CONFIG"])
        print(f"   æ£€æµ‹å®Œæˆ: {len(isolation_suspects)} ä¸ªå¯ç–‘æ ·æœ¬")

        # 7. é›†æˆå¤šç®—æ³•ç»“æœ
        print("\nğŸ¤ æ­¥éª¤ 7/9: é›†æˆå¤šç®—æ³•ç»“æœ")
        df_ensemble = ensemble_decision(
            y_true, cleanlab_suspects, cleanlab_scores,
            kmeans_suspects, kmeans_scores,
            isolation_suspects, isolation_scores, CONFIG["ENSEMBLE_CONFIG"]
        )
        print(f"   é›†æˆå®Œæˆ: {len(df_ensemble)} ä¸ªæ ·æœ¬å¤„ç†")

        # 8. å¯¼å‡ºå¢å¼ºç‰ˆç»“æœ
        print("\nğŸ’¾ æ­¥éª¤ 8/9: å¯¼å‡ºç»“æœ")
        export_enhanced_results(
            df_ensemble,
            img_paths,
            label_map,
            y_true,
            CONFIG["OUTPUT_DIR"],
            CONFIG["SUSPECTS_DIR"],
            CONFIG["CLEAN_DIR"],
            CONFIG["SAVE_IMGS"]
        )

        # 9. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        if CONFIG["ENABLE_VISUALIZATION"]:
            print("\nğŸ¨ æ­¥éª¤ 9/9: ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
            try:
                visualizer = AnomalyDetectionVisualizer(CONFIG["VISUALIZATION_DIR"])
                visualizer.visualize_all(df_ensemble, img_paths)
                print(f"   å¯è§†åŒ–å®Œæˆ: å›¾è¡¨å·²ä¿å­˜è‡³ {CONFIG['VISUALIZATION_DIR']}")
            except Exception as e:
                print(f"   âš ï¸ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
                print("   ç¨‹åºå°†ç»§ç»­æ‰§è¡Œ...")
        else:
            print("\nâ­ï¸ æ­¥éª¤ 9/9: è·³è¿‡å¯è§†åŒ–ï¼ˆå·²ç¦ç”¨ï¼‰")

        # ç»Ÿè®¡ä¿¡æ¯
        suspects_count = len(df_ensemble[df_ensemble['is_suspect'] == True])
        clean_count = len(df_ensemble[df_ensemble['is_suspect'] == False])

        print(f"\nğŸ“ˆ æ¸…æ´—ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {len(df_ensemble)}")
        print(f"   å¯ç–‘æ ·æœ¬: {suspects_count} ({suspects_count / len(df_ensemble) * 100:.1f}%)")
        print(f"   å¹²å‡€æ ·æœ¬: {clean_count} ({clean_count / len(df_ensemble) * 100:.1f}%)")

        total_time = time.time() - start_time
        print(f"\nğŸ‰ å¢å¼ºç‰ˆæ¸…æ´—å®Œæˆï¼æ€»è€—æ—¶: {total_time:.1f}ç§’")

    except KeyboardInterrupt:
        print(f"\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        # æ¸…ç†æ—¥å¿—ç³»ç»Ÿ
        cleanup_logging(dual_output, original_stdout, log_file)


if __name__ == "__main__":
    main()