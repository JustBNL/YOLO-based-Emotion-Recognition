#!/usr/bin/env python
"""
eval_classifier.py â€“ æƒ…ç»ªåˆ†ç±»æ¨¡å‹è¯„ä¼°è„šæœ¬ï¼ˆæ”¯æŒä¸­æ–‡æ§åˆ¶å°è¾“å‡ºï¼Œå›¾åƒç±»å‹å…¼å®¹ï¼Œè‡ªåŠ¨ä¿å­˜æŠ¥å‘Šï¼ŒBNè‡ªåŠ¨æ ¡å‡†ï¼‰
"""

from __future__ import annotations
from pathlib import Path
import numpy as np
import cv2
from ultralytics import YOLO
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import seaborn as sns
import os
import csv
import time
from collections import Counter, defaultdict
import torch
import torch.nn as nn
from tqdm import tqdm

# ----------------------------------------------------------------------
# CONFIG
# ----------------------------------------------------------------------
CONFIG = {
    "cls_run": "yolo11s-cls_20250624-232219-clean-cbam-new",
    "data_dir": "datasets/cls/processed/KDEF-2",
    "img_size": 224,
    "device": "0",
    "names": ["angry", "disgust", "fear", "happy", "sad", "surprise", "neutral"],
    "font": cv2.FONT_HERSHEY_SIMPLEX,
    "report_save": True,
    "report_dir": "eval_result/cls",
    "analyze_classes": ["angry", "disgust", "fear", "happy", "sad", "surprise", "neutral"],

    # BNæ ¡å‡†ç›¸å…³é…ç½®
    "bn_calibration": {
        "enable": False,  # æ˜¯å¦å¯ç”¨BNæ ¡å‡†
        "calibration_samples": -1,  # ç”¨äºæ ¡å‡†çš„æ ·æœ¬æ•°é‡ï¼ˆ-1è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ ·æœ¬ï¼‰
        "calibration_batch_size": 64,  # æ ¡å‡†æ—¶çš„æ‰¹å¤„ç†å¤§å°
        "momentum": 0.1,  # BNå±‚çš„åŠ¨é‡å‚æ•°
        "eps": 1e-5,  # BNå±‚çš„epsilonå‚æ•°
        "verbose": True  # æ˜¯å¦æ˜¾ç¤ºæ ¡å‡†è¿‡ç¨‹
    }
}

# ----------------------------------------------------------------------
SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent


# ----------------------------------------------------------------------
def load_data(data_dir: Path) -> tuple[list[np.ndarray], list[int], list[str]]:
    images, labels, image_names = [], [], []
    data_dir = PROJECT_ROOT / data_dir if not Path(data_dir).is_absolute() else Path(data_dir)
    if not data_dir.exists():
        raise FileNotFoundError(f"âŒ è¯„ä¼°æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {data_dir}")

    class_dirs = sorted([d for d in data_dir.iterdir() if d.is_dir()])
    for idx, class_dir in enumerate(class_dirs):
        for ext in ["*.jpg", "*.png", "*.jpeg"]:
            for img_path in class_dir.rglob(ext):
                img = cv2.imread(str(img_path))
                if img is None:
                    continue
                img = cv2.resize(img, (CONFIG["img_size"], CONFIG["img_size"]))
                images.append(img)
                labels.append(idx)
                image_names.append(str(img_path))

    return images, labels, image_names


# ----------------------------------------------------------------------
def reset_bn_stats(model: torch.nn.Module) -> None:
    """é‡ç½®æ‰€æœ‰BatchNormå±‚çš„ç»Ÿè®¡ä¿¡æ¯"""
    for module in model.modules():
        if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
            module.reset_running_stats()


def calibrate_bn_stats(model: torch.nn.Module, calibration_data: list,
                       batch_size: int = 32, device: str = "cpu", verbose: bool = True) -> None:
    """
    ä½¿ç”¨æ ¡å‡†æ•°æ®é‡æ–°è®¡ç®—BatchNormå±‚çš„è¿è¡Œç»Ÿè®¡ä¿¡æ¯

    Args:
        model: éœ€è¦æ ¡å‡†çš„æ¨¡å‹
        calibration_data: æ ¡å‡†æ•°æ®åˆ—è¡¨
        batch_size: æ‰¹å¤„ç†å¤§å°
        device: è®¾å¤‡
        verbose: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
    """
    if verbose:
        print("ğŸ”§ å¼€å§‹BNæ ¡å‡†...")

    # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼ä»¥æ›´æ–°BNç»Ÿè®¡ä¿¡æ¯
    model.train()

    # å…ˆé‡ç½®æ‰€æœ‰BNå±‚çš„ç»Ÿè®¡ä¿¡æ¯
    reset_bn_stats(model)

    # æ‰¹å¤„ç†æ ¡å‡†æ•°æ®
    num_batches = (len(calibration_data) + batch_size - 1) // batch_size
    progress_bar = tqdm(range(num_batches), desc="BNæ ¡å‡†è¿›åº¦") if verbose else range(num_batches)

    with torch.no_grad():
        for batch_idx in progress_bar:
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(calibration_data))
            batch_data = calibration_data[start_idx:end_idx]

            # è½¬æ¢ä¸ºtensorå¹¶ç§»åˆ°æŒ‡å®šè®¾å¤‡
            batch_tensor = torch.stack([
                torch.from_numpy(img.transpose(2, 0, 1)).float() / 255.0
                for img in batch_data
            ]).to(device)

            # å‰å‘ä¼ æ’­æ›´æ–°BNç»Ÿè®¡ä¿¡æ¯
            try:
                # å¯¹äºYOLOæ¨¡å‹ï¼Œç›´æ¥è°ƒç”¨æ¨¡å‹çš„backboneéƒ¨åˆ†
                if hasattr(model, 'model'):
                    _ = model.model(batch_tensor)
                else:
                    _ = model(batch_tensor)
            except Exception as e:
                if verbose:
                    print(f"âš ï¸  æ‰¹æ¬¡ {batch_idx} æ ¡å‡†æ—¶å‡ºç°è­¦å‘Š: {e}")
                continue

    # æ¢å¤ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()

    if verbose:
        print("âœ… BNæ ¡å‡†å®Œæˆ")


def apply_bn_calibration(model: YOLO, calibration_images: list, config: dict) -> None:
    """
    å¯¹YOLOæ¨¡å‹åº”ç”¨BNæ ¡å‡†

    Args:
        model: YOLOæ¨¡å‹å®ä¾‹
        calibration_images: ç”¨äºæ ¡å‡†çš„å›¾åƒåˆ—è¡¨
        config: æ ¡å‡†é…ç½®
    """
    cal_config = config.get("bn_calibration", {})

    if not cal_config.get("enable", False):
        return

    # è·å–æ ¡å‡†æ ·æœ¬
    num_samples = cal_config.get("calibration_samples", 200)
    if num_samples == -1 or num_samples >= len(calibration_images):
        cal_data = calibration_images
    else:
        # éšæœºé‡‡æ ·
        indices = np.random.choice(len(calibration_images), num_samples, replace=False)
        cal_data = [calibration_images[i] for i in indices]

    if cal_config.get("verbose", True):
        print(f"ğŸ“Š ä½¿ç”¨ {len(cal_data)} ä¸ªæ ·æœ¬è¿›è¡ŒBNæ ¡å‡†")

    # è·å–æ¨¡å‹çš„PyTorchæ¨¡å‹
    torch_model = model.model
    device = next(torch_model.parameters()).device

    # æ‰§è¡Œæ ¡å‡†
    calibrate_bn_stats(
        model=torch_model,
        calibration_data=cal_data,
        batch_size=cal_config.get("calibration_batch_size", 32),
        device=device,
        verbose=cal_config.get("verbose", True)
    )


# ----------------------------------------------------------------------
def evaluate() -> None:
    cfg = CONFIG
    model_path = PROJECT_ROOT / "runs/cls/train" / cfg["cls_run"] / "weights" / "best.pt"
    if not model_path.exists():
        raise FileNotFoundError(f"âŒ æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}")

    data_dir_abs = PROJECT_ROOT / cfg["data_dir"] if not Path(cfg["data_dir"]).is_absolute() else Path(cfg["data_dir"])
    rel_data_dir = data_dir_abs.relative_to(PROJECT_ROOT).as_posix().replace("/", "_")

    # åœ¨è¾“å‡ºç›®å½•åä¸­æ ‡è®°æ˜¯å¦ä½¿ç”¨äº†BNæ ¡å‡†
    bn_suffix = "_bn_cal" if cfg.get("bn_calibration", {}).get("enable", False) else ""
    out_dir = PROJECT_ROOT / cfg["report_dir"] / f"{cfg['cls_run']}__{rel_data_dir}{bn_suffix}"
    out_dir.mkdir(parents=True, exist_ok=True)
    error_img_dir = out_dir / "error_images"
    error_img_dir.mkdir(parents=True, exist_ok=True)

    # åŠ è½½æ¨¡å‹
    print("ğŸš€ åŠ è½½æ¨¡å‹...")
    model = YOLO(str(model_path))

    # åŠ è½½æ•°æ®
    print("ğŸ“‚ åŠ è½½è¯„ä¼°æ•°æ®...")
    images, gt_labels, image_names = load_data(cfg["data_dir"])
    print(f"ğŸ“Š åŠ è½½äº† {len(images)} å¼ å›¾åƒ")

    # åº”ç”¨BNæ ¡å‡†
    if cfg.get("bn_calibration", {}).get("enable", False):
        apply_bn_calibration(model, images, cfg)

    name2idx = {name: i for i, name in enumerate(cfg["names"])}
    analyze_class_idxs = [name2idx[name] for name in cfg.get("analyze_classes", [])]

    # å¼€å§‹æ¨ç†
    print("ğŸ” å¼€å§‹æ¨¡å‹æ¨ç†...")
    start_time = time.time()
    pred_labels = []
    top3_preds = []
    all_probs = []

    # æ·»åŠ è¿›åº¦æ¡
    for i, img in enumerate(tqdm(images, desc="æ¨ç†è¿›åº¦")):
        pred = model(img, imgsz=cfg["img_size"], device=cfg["device"], verbose=False)[0]
        idx = int(pred.probs.top1)
        pred_labels.append(idx)
        probs = pred.probs.data.tolist() if hasattr(pred.probs, 'data') else pred.probs.tolist()
        top3 = sorted(range(len(probs)), key=lambda i: -probs[i])[:3]
        top3_preds.append(top3)
        all_probs.append(probs)

    elapsed = time.time() - start_time
    print(f"â±ï¸  æ¨ç†å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f} ç§’")

    print("\nğŸ“Š åˆ†ç±»æŠ¥å‘Š:")
    report = classification_report(gt_labels, pred_labels, target_names=cfg["names"], digits=4)
    print(report)

    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = sum(1 for gt, pred in zip(gt_labels, pred_labels) if gt == pred) / len(gt_labels)
    print(f"ğŸ¯ æ€»ä½“å‡†ç¡®ç‡: {accuracy:.4f}")

    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(gt_labels, pred_labels)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=cfg["names"])
    fig1, ax1 = plt.subplots(figsize=(8, 6))
    disp.plot(ax=ax1, cmap="Blues", xticks_rotation=45)
    ax1.set_title("Confusion Matrix")
    fig1.tight_layout()

    # ç»˜åˆ¶å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ
    cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)
    fig2, ax2 = plt.subplots(figsize=(8, 6))
    sns.heatmap(cm_norm, annot=True, fmt=".2f", xticklabels=cfg["names"], yticklabels=cfg["names"], cmap="Blues")
    ax2.set_title("Normalized Confusion Matrix")
    ax2.set_ylabel("True Label")
    ax2.set_xlabel("Predicted Label")
    fig2.tight_layout()

    if cfg.get("report_save", False):
        # ä¿å­˜é…ç½®å’Œç»“æœ
        with open(out_dir / "report.txt", "w", encoding="utf-8") as f:
            f.write("æ¨¡å‹è¯„ä¼°é…ç½®:\n")
            for k, v in cfg.items():
                f.write(f"{k}: {v}\n")
            f.write(f"\næ€»è€—æ—¶: {elapsed:.2f} ç§’\n")
            f.write(f"æ€»ä½“å‡†ç¡®ç‡: {accuracy:.4f}\n")
            f.write(f"BNæ ¡å‡†çŠ¶æ€: {'å¯ç”¨' if cfg.get('bn_calibration', {}).get('enable', False) else 'ç¦ç”¨'}\n")
            f.write("\nåˆ†ç±»æŠ¥å‘Š:\n")
            f.write(report)

        # ä¿å­˜å›¾åƒ
        fig1.savefig(out_dir / "confusion_matrix.png", dpi=300, bbox_inches='tight')
        fig2.savefig(out_dir / "confusion_matrix_normalized.png", dpi=300, bbox_inches='tight')

        # è®¡ç®—æ¯ç±»å‡†ç¡®ç‡
        class_counts = np.bincount(gt_labels, minlength=len(cfg["names"]))
        correct = [(np.array(gt_labels) == i).astype(int) & (np.array(pred_labels) == i).astype(int) for i in
                   range(len(cfg["names"]))]
        correct_counts = [int(c.sum()) for c in correct]
        class_acc = [c / total if total > 0 else 0.0 for c, total in zip(correct_counts, class_counts)]

        # ç»˜åˆ¶æ¯ç±»å‡†ç¡®ç‡
        plt.figure(figsize=(10, 6))
        bars = plt.bar(cfg["names"], class_acc, color='skyblue', alpha=0.7)
        plt.ylim(0, 1)
        plt.ylabel("Accuracy")
        plt.xlabel("Classes")
        plt.title("Per-Class Accuracy")
        plt.xticks(rotation=45)

        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, acc in zip(bars, class_acc):
            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                     f'{acc:.3f}', ha='center', va='bottom')

        plt.tight_layout()
        plt.savefig(out_dir / "per_class_accuracy.png", dpi=300, bbox_inches='tight')

        # ä¿å­˜CSVæ ¼å¼çš„å‡†ç¡®ç‡æ•°æ®
        csv_path = out_dir / "class_accuracy.csv"
        with open(csv_path, "w", newline="", encoding="utf-8") as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(["Class", "Total", "Correct", "Accuracy"])
            for name, total, corr, acc in zip(cfg["names"], class_counts, correct_counts, class_acc):
                writer.writerow([name, total, corr, f"{acc:.4f}"])

        # é”™è¯¯åˆ†æ
        error_counter = defaultdict(int)
        error_detail_path = out_dir / "error_detail.txt"
        with open(error_detail_path, "w", encoding="utf-8") as f:
            f.write("é”™è¯¯æ ·æœ¬åˆ†æ (åŒ…å«é¢„æµ‹ç±»åˆ«):\n")
            f.write(f"BNæ ¡å‡†çŠ¶æ€: {'å¯ç”¨' if cfg.get('bn_calibration', {}).get('enable', False) else 'ç¦ç”¨'}\n\n")

            for i, (true, pred, top3, img_path, probs) in enumerate(
                    zip(gt_labels, pred_labels, top3_preds, image_names, all_probs)):
                if true in analyze_class_idxs and true != pred:
                    rank = top3.index(true) + 1 if true in top3 else "N/A"
                    tag = f"Top{rank}_correct_but_not_top1 [{cfg['names'][true]}]"
                    error_counter[tag] += 1
                    f.write(
                        f"{img_path} -> GT: {cfg['names'][true]} | Pred: {cfg['names'][pred]} | Top3: {[cfg['names'][k] for k in top3]}\n")

                    # ä¿å­˜é”™è¯¯æ ·æœ¬å›¾åƒ
                    img = images[i].copy()
                    prob = probs[pred] if pred < len(probs) else 0.0
                    cv2.putText(img, f"GT:{cfg['names'][true]}", (2, 14), cfg['font'], 0.4, (0, 0, 255), 1)
                    cv2.putText(img, f"Pred:{cfg['names'][pred]}", (2, 28), cfg['font'], 0.4, (255, 0, 0), 1)
                    cv2.putText(img, f"Conf:{prob:.2f}", (2, 42), cfg['font'], 0.4, (0, 128, 0), 1)
                    cv2.imwrite(str(error_img_dir / f"{Path(img_path).stem}_err.jpg"), img)

        # ä¿å­˜é”™è¯¯ç»Ÿè®¡
        with open(out_dir / "error_analysis.txt", "w", encoding="utf-8") as f:
            f.write("Top-k é”™è¯¯åˆ†æ (ä»…åˆ†æç›®æ ‡ç±»åˆ«):\n")
            f.write(f"BNæ ¡å‡†çŠ¶æ€: {'å¯ç”¨' if cfg.get('bn_calibration', {}).get('enable', False) else 'ç¦ç”¨'}\n\n")
            for k in sorted(error_counter):
                f.write(f"{k}: {error_counter[k]}\n")

        print(f"âœ… æŠ¥å‘Šä¿å­˜äº: {out_dir.resolve()}")

        # è¾“å‡ºBNæ ¡å‡†ä¿¡æ¯
        if cfg.get("bn_calibration", {}).get("enable", False):
            cal_config = cfg["bn_calibration"]
            print(f"ğŸ”§ BNæ ¡å‡†å·²å¯ç”¨:")
            print(f"   - æ ¡å‡†æ ·æœ¬æ•°: {cal_config.get('calibration_samples', 200)}")
            print(f"   - æ‰¹å¤„ç†å¤§å°: {cal_config.get('calibration_batch_size', 32)}")
    else:
        plt.show()


# ----------------------------------------------------------------------
if __name__ == "__main__":
    evaluate()