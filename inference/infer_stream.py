from __future__ import annotations

from pathlib import Path
from threading import Thread
from queue import Queue, Empty
from typing import List, Union, Tuple, Any
import time
import sys

import cv2
import numpy as np
import torch
from tqdm import tqdm
from ultralytics import YOLO

# ----------------------------------------------------------------------
# å…¨å±€é…ç½®
# ----------------------------------------------------------------------
CONFIG: dict[str, Any] = {
    "det_run": "face-yolo11n_20250614-1807482",
    "cls_run": "yolo11s-cls_20250625-201726-new-clean-RFAConv",

    # è¾“å…¥ / è¾“å‡º
    "input": 2,                       # è·¯å¾„ | ç›®å½• | int(æ‘„åƒå¤´) | URL
    "output_dir": "output/videos",    # è¾“å‡ºè§†é¢‘ç›®å½•

    # æ¨ç†å‚æ•°
    "device": "cuda",                 # "cpu" | "0" | "0,1"  # æ”¹ä¸º cpu æ›´å®‰å…¨
    "img_size": 224,                  # åˆ†ç±»æ¨¡å‹è¾“å…¥å°ºå¯¸ (æ­£æ–¹å½¢)
    "conf_thres": 0.5,
    "half": False,                    # é»˜è®¤å…³é—­åŠç²¾åº¦ï¼Œé¿å…å…¼å®¹æ€§é—®é¢˜

    # è¿è¡Œæ—¶é€‰é¡¹
    "display": True,                  # æ˜¯å¦æ˜¾ç¤ºçª—å£
    "save_video": True,
    "overwrite_output": False,
    "font": cv2.FONT_HERSHEY_SIMPLEX,
    "colors": [                       # ç»˜åˆ¶æ¡†é¢œè‰²å¾ªç¯
        (255, 0, 0), (0, 255, 0), (0, 128, 255), (255, 0, 255),
        (0, 255, 255), (255, 255, 0), (128, 0, 128),
    ],
    "names": ["angry", "disgust", "fear", "happy", "sad", "neutral", "surprise"],
    "label_display": "label",        # "label" | "code"

    # FPS è°ƒä¼˜
    "skip_frames": 1,                 # é™æ€è·³å¸§ï¼šæ¯ N å¸§å¤„ç†ä¸€å¸§
    "auto_skip": True,                # åŠ¨æ€è°ƒèŠ‚è·³å¸§
    "target_fps": 30,                # ç›®æ ‡å®æ—¶ FPS

    # çº¿ç¨‹åŒ– I/O
    "queue_size": 16,                # é¢„è¯»å–ç¼“å†²å¸§æ•°
}

SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent

# æé«˜ matmul ç²¾åº¦ï¼ˆTransformer æ¨¡å‹å¸¸ç”¨ï¼‰
torch.set_float32_matmul_precision("high")

# ----------------------------------------------------------------------
# å·¥å…·ç±»
# ----------------------------------------------------------------------

class FrameGrabber(Thread):
    """åå°è¿ç»­è¯»å–è§†é¢‘å¸§ï¼Œé¿å…æ¨ç†çº¿ç¨‹è¢« I/O é˜»å¡ã€‚"""

    def __init__(self, src: Union[str, int], queue_size: int = 16):
        super().__init__(daemon=True)
        self.cap = cv2.VideoCapture(src)
        if not self.cap.isOpened():
            raise RuntimeError(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘æº: {src}")
        self.q: Queue[np.ndarray] = Queue(maxsize=queue_size)
        self.stopped = False

    def run(self):
        while not self.stopped:
            if not self.q.full():
                ret, frame = self.cap.read()
                if not ret:
                    self.stop()
                    break
                if not self.q.full():  # å†æ¬¡æ£€æŸ¥ï¼Œé¿å…ç«æ€æ¡ä»¶
                    try:
                        self.q.put(frame, timeout=0.1)
                    except:
                        pass
            else:
                time.sleep(0.001)  # è®©å‡º CPU

    def read(self, timeout: float = 1.0) -> Tuple[bool, np.ndarray | None]:
        try:
            frame = self.q.get(timeout=timeout)
            return True, frame
        except Empty:
            return False, None

    def more(self) -> bool:
        return not self.q.empty() or not self.stopped

    def stop(self):
        self.stopped = True
        if self.cap:
            self.cap.release()


# ----------------------------------------------------------------------
# æ¨¡å‹åŠ è½½
# ----------------------------------------------------------------------

def load_models(cfg: dict) -> tuple[YOLO, YOLO]:
    """åŠ è½½æ£€æµ‹ä¸åˆ†ç±»æ¨¡å‹ã€‚"""
    cfg["det_path"] = PROJECT_ROOT / f"runs/det/train/{cfg['det_run']}/weights/best.pt"
    cfg["cls_path"] = PROJECT_ROOT / f"runs/cls/train/{cfg['cls_run']}/weights/best.pt"

    for key in ("det_path", "cls_path"):
        if not cfg[key].exists():
            print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {cfg[key]}")
            print("å°è¯•ä½¿ç”¨é»˜è®¤ YOLO æ¨¡å‹...")
            # ä½¿ç”¨é»˜è®¤æ¨¡å‹ä½œä¸ºåå¤‡
            if key == "det_path":
                cfg[key] = "yolo11n.pt"  # äººè„¸æ£€æµ‹ç”¨é»˜è®¤æ¨¡å‹
            else:
                cfg[key] = "yolo11s-cls.pt"  # åˆ†ç±»ç”¨é»˜è®¤æ¨¡å‹

    print("ğŸ”¹ æ­£åœ¨åŠ è½½æ¨¡å‹ â€¦")
    try:
        det = YOLO(str(cfg["det_path"]))
        cls = YOLO(str(cfg["cls_path"]))
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("ä½¿ç”¨é»˜è®¤æ¨¡å‹...")
        det = YOLO("yolo11n.pt")
        cls = YOLO("yolo11s-cls.pt")

    device = cfg["device"]
    det.to(device)
    cls.to(device)

    if cfg["half"] and torch.cuda.is_available() and device != "cpu":
        try:
            det.model.half()
            cls.model.half()
        except:
            print("âš ï¸ åŠç²¾åº¦è½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨å…¨ç²¾åº¦")
            cfg["half"] = False

    return det, cls


# ----------------------------------------------------------------------
# è§†é¢‘æºè§£æ
# ----------------------------------------------------------------------

def get_video_sources(src: Union[str, int, Path]) -> List[Union[int, str]]:
    """æ ¹æ®è¾“å…¥è¿”å›è§†é¢‘æºåˆ—è¡¨ã€‚"""
    if isinstance(src, int):
        return [src]
    src_path = Path(str(src))
    if src_path.exists():
        if src_path.is_file():
            return [str(src_path)]
        if src_path.is_dir():
            vids = sorted([p for p in src_path.iterdir() if p.suffix.lower() in {'.mp4', '.avi', '.mov', '.mkv'}])
            if not vids:
                print(f"âš ï¸ ç›®å½•ä¸ºç©º: {src_path}")
                return []
            return [str(v) for v in vids]
    # å…¶ä½™æƒ…å†µæŒ‰ URL/æœªçŸ¥è·¯å¾„å¤„ç†
    return [str(src)]


# ----------------------------------------------------------------------
# ç»˜åˆ¶è¾…åŠ©
# ----------------------------------------------------------------------

def draw_label(img: np.ndarray, text: str, tl: Tuple[int, int], color: Tuple[int, int, int]) -> None:
    """åœ¨ç›®æ ‡æ¡†å·¦ä¸Šç»˜åˆ¶å¸¦èƒŒæ™¯çš„æ–‡æœ¬ã€‚"""
    font_scale = max(img.shape[1] / 800.0, 0.6)
    thickness = max(1, int(font_scale * 2))
    (w, h), _ = cv2.getTextSize(text, CONFIG["font"], font_scale, thickness)
    top_left = tl
    bottom_right = (tl[0] + w + 10, tl[1] - h - 10)
    cv2.rectangle(img, top_left, bottom_right, color, -1, cv2.LINE_AA)
    cv2.putText(img, text, (tl[0] + 5, tl[1] - 5), CONFIG["font"], font_scale, (0, 0, 0), thickness + 1, cv2.LINE_AA)


# ----------------------------------------------------------------------
# è§†é¢‘å†™å…¥
# ----------------------------------------------------------------------

def unique_path(path: Path) -> Path:
    """è‹¥æ–‡ä»¶å·²å­˜åœ¨åˆ™è‡ªåŠ¨è¿½åŠ ç´¢å¼•é¿å…è¦†ç›–ã€‚"""
    if CONFIG["overwrite_output"] or not path.exists():
        return path
    stem, suf = path.stem, path.suffix
    for i in range(1, 1000):
        cand = path.with_name(f"{stem}_{i}{suf}")
        if not cand.exists():
            return cand
    return path


def create_writer(cfg: dict, source: Union[str, int], frame: np.ndarray) -> cv2.VideoWriter | None:
    """æ ¹æ®é¦–å¸§åˆ†è¾¨ç‡åˆ›å»º VideoWriterã€‚"""
    if not cfg["save_video"]:
        return None
    try:
        out_dir = SCRIPT_DIR / cfg["output_dir"]
        out_dir.mkdir(parents=True, exist_ok=True)

        name = f"camera_{source}.mp4" if isinstance(source, int) else f"{Path(str(source)).stem}.mp4"
        out_path = unique_path(out_dir / name)
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        h, w = frame.shape[:2]
        writer = cv2.VideoWriter(str(out_path), fourcc, cfg["target_fps"], (w, h))
        return writer
    except Exception as e:
        print(f"âš ï¸ åˆ›å»ºè§†é¢‘å†™å…¥å™¨å¤±è´¥: {e}")
        return None


# ----------------------------------------------------------------------
# æ¨ç†ä¸»å¾ªç¯
# ----------------------------------------------------------------------

def process_stream(cfg: dict, source: Union[int, str], det: YOLO, cls: YOLO) -> None:
    try:
        grabber = FrameGrabber(source, queue_size=cfg["queue_size"])
        grabber.start()
    except Exception as e:
        print(f"âŒ æ— æ³•å¯åŠ¨å¸§é‡‡é›†å™¨: {e}")
        return

    # åˆå§‹åŒ–è®¡æ—¶
    prev_time = time.time()
    avg_fps = 0.0
    frame_count = 0

    # ç­‰å¾…é¦–å¸§ä»¥ç¡®å®šåˆ†è¾¨ç‡
    ok, frame = grabber.read()
    if not ok:
        print("âš ï¸ æ— æ³•è¯»å–é¦–å¸§ï¼Œè·³è¿‡è¯¥æºã€‚")
        grabber.stop()
        return

    writer = create_writer(cfg, source, frame)

    pbar = tqdm(total=0, position=0, unit="fps", bar_format="{desc}")

    try:
        while grabber.more():
            ok, frame = grabber.read()
            if not ok:
                break

            frame_count += 1
            # é™æ€è·³å¸§
            if cfg["skip_frames"] > 1 and frame_count % cfg["skip_frames"] != 0:
                continue

            # ------------------------------------
            # 1. äººè„¸æ£€æµ‹
            # ------------------------------------
            try:
                det_res = det.predict(frame, conf=cfg["conf_thres"], device=cfg["device"], verbose=False)[0]
                boxes = det_res.boxes.xyxy.cpu().numpy() if det_res.boxes is not None else []
            except Exception as e:
                print(f"âš ï¸ æ£€æµ‹å¤±è´¥: {e}")
                boxes = []

            faces = []
            coords = []
            for box in boxes:
                x1, y1, x2, y2 = map(int, box[:4])
                x1, y1 = max(0, x1), max(0, y1)
                x2, y2 = min(frame.shape[1] - 1, x2), min(frame.shape[0] - 1, y2)
                if x2 <= x1 or y2 <= y1:  # æ— æ•ˆæ¡†
                    continue
                face = frame[y1:y2, x1:x2]
                if face.size == 0:
                    continue
                faces.append(face)
                coords.append((x1, y1, x2, y2))

            # ------------------------------------
            # 2. æ‰¹é‡è¡¨æƒ…åˆ†ç±»
            # ------------------------------------
            labels = []
            if faces:
                try:
                    batch = []
                    for f in faces:
                        img = cv2.resize(f, (cfg["img_size"], cfg["img_size"]))
                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                        img = img.transpose(2, 0, 1) / 255.0
                        batch.append(img)
                    batch_tensor = torch.from_numpy(np.stack(batch)).float()
                    if cfg["half"] and torch.cuda.is_available() and cfg["device"] != "cpu":
                        batch_tensor = batch_tensor.half()
                    batch_tensor = batch_tensor.to(cfg["device"])

                    cls_res = cls.predict(batch_tensor, device=cfg["device"], verbose=False)
                    # ç±»åˆ«æƒé‡ï¼ˆä½ å¯ä»¥è°ƒæ•´è¿™äº›å€¼ï¼‰
                    class_bias = torch.tensor([1.6, 1.7, 1.5, 0.9, 0.2, 2.8, 5.0], device=cfg["device"])

                    for res in cls_res:
                        prob = res.probs
                        if prob is None:
                            labels.append("?")
                            continue

                        logits = prob.data  # shape: (num_classes,)
                        if logits is None or not isinstance(logits, torch.Tensor):
                            labels.append("?")
                            continue

                        # åŠ æƒå¹¶softmax
                        adjusted_logits = logits * class_bias
                        adjusted_probs = torch.nn.functional.softmax(adjusted_logits, dim=0)
                        idx = int(torch.argmax(adjusted_probs))

                        if 0 <= idx < len(cfg["names"]):
                            label = cfg["names"][idx]
                        else:
                            label = "unknown"
                        labels.append(label)

                except Exception as e:
                    print(f"âš ï¸ åˆ†ç±»å¤±è´¥: {e}")
                    labels = ["error"] * len(faces)

            # ------------------------------------
            # 3. ç»˜åˆ¶ç»“æœ
            # ------------------------------------
            for (x1, y1, x2, y2), label in zip(coords, labels):
                try:
                    idx = cfg["names"].index(label) if label in cfg["names"] else 0
                    color = cfg["colors"][idx % len(cfg["colors"])]
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                    draw_label(frame, label, (x1, y1), color)
                except Exception as e:
                    print(f"âš ï¸ ç»˜åˆ¶å¤±è´¥: {e}")

            # ------------------------------------
            # 4. FPS & è¾“å‡º
            # ------------------------------------
            now = time.time()
            dt = now - prev_time
            prev_time = now
            fps = 1.0 / dt if dt > 0 else 0.0
            avg_fps = 0.9 * avg_fps + 0.1 * fps if avg_fps else fps
            pbar.set_description(f"{source} | FPS: {avg_fps:.1f}")

            if writer:
                try:
                    writer.write(frame)
                except:
                    pass
            if cfg["display"]:
                cv2.imshow(str(source), frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

            # åŠ¨æ€è°ƒèŠ‚è·³å¸§
            if cfg["auto_skip"] and avg_fps:
                if avg_fps < cfg["target_fps"] * 0.9:
                    cfg["skip_frames"] = min(cfg["skip_frames"] + 1, 5)
                elif avg_fps > cfg["target_fps"] * 1.1:
                    cfg["skip_frames"] = max(cfg["skip_frames"] - 1, 1)

    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    finally:
        # æ¸…ç†
        grabber.stop()
        if writer:
            writer.release()
        cv2.destroyAllWindows()
        pbar.close()


# ----------------------------------------------------------------------
# é«˜å±‚ API
# ----------------------------------------------------------------------

def run_infer(*, input_source: Union[int, str] | None = None, target_fps: int | None = None) -> None:
    """ä¾›å¤–éƒ¨è„šæœ¬è°ƒç”¨çš„ç®€æ˜“å…¥å£ã€‚"""
    if input_source is not None:
        CONFIG["input"] = input_source
    if target_fps is not None:
        CONFIG["target_fps"] = target_fps

    try:
        det_model, cls_model = load_models(CONFIG)
        sources = get_video_sources(CONFIG["input"])
        if not sources:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è§†é¢‘æº")
            return
        for src in sources:
            process_stream(CONFIG, src, det_model, cls_model)
    except Exception as e:
        print(f"âŒ è¿è¡Œå¤±è´¥: {e}")


# ----------------------------------------------------------------------
# CLI å…¥å£
# ----------------------------------------------------------------------

if __name__ == "__main__":
    run_infer()