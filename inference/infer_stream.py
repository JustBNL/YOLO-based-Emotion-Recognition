from __future__ import annotations

from pathlib import Path
from threading import Thread
from queue import Queue, Empty
from typing import List, Union, Tuple, Any
import time
import sys

import cv2
import numpy as np
import torch
from tqdm import tqdm
from ultralytics import YOLO

# ----------------------------------------------------------------------
# å…¨å±€é…ç½®
# ----------------------------------------------------------------------
CONFIG: dict[str, Any] = {
    "det_run": "face-yolo11n_20250614-1807482",
    "cls_run": "yolo11s-cls_20250620-114505",

    # è¾“å…¥ / è¾“å‡º
    "input": 1,                       # è·¯å¾„ | ç›®å½• | int(æ‘„åƒå¤´) | URL
    "output_dir": "output/videos",    # è¾“å‡ºè§†é¢‘ç›®å½•

    # æ¨ç†å‚æ•°
    "device": "0",                   # "cpu" | "0" | "0,1"
    "img_size": 224,                  # åˆ†ç±»æ¨¡å‹è¾“å…¥å°ºå¯¸ (æ­£æ–¹å½¢)
    "conf_thres": 0.5,
    "half": True,                     # CUDA ä¸Šä½¿ç”¨åŠç²¾åº¦

    # è¿è¡Œæ—¶é€‰é¡¹
    "display": True,                  # æ˜¯å¦æ˜¾ç¤ºçª—å£
    "save_video": True,
    "overwrite_output": False,
    "font": cv2.FONT_HERSHEY_SIMPLEX,
    "colors": [                       # ç»˜åˆ¶æ¡†é¢œè‰²å¾ªç¯
        (255, 0, 0), (0, 255, 0), (0, 128, 255), (255, 0, 255),
        (0, 255, 255), (255, 255, 0), (128, 0, 128),
    ],
    "names": ["angry", "disgust", "fear", "happy", "sad", "surprise", "neutral"],
    "label_display": "label",        # "label" | "code"

    # FPS è°ƒä¼˜
    "skip_frames": 1,                 # é™æ€è·³å¸§ï¼šæ¯ N å¸§å¤„ç†ä¸€å¸§
    "auto_skip": True,                # åŠ¨æ€è°ƒèŠ‚è·³å¸§
    "target_fps": 30,                # ç›®æ ‡å®æ—¶ FPS

    # çº¿ç¨‹åŒ– I/O
    "queue_size": 16,                # é¢„è¯»å–ç¼“å†²å¸§æ•°
}

SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent

# æé«˜ matmul ç²¾åº¦ï¼ˆTransformer æ¨¡å‹å¸¸ç”¨ï¼‰
torch.set_float32_matmul_precision("high")

# ----------------------------------------------------------------------
# å·¥å…·ç±»
# ----------------------------------------------------------------------

class FrameGrabber(Thread):
    """åå°è¿ç»­è¯»å–è§†é¢‘å¸§ï¼Œé¿å…æ¨ç†çº¿ç¨‹è¢« I/O é˜»å¡ã€‚"""

    def __init__(self, src: Union[str, int], queue_size: int = 16):
        super().__init__(daemon=True)
        self.cap = cv2.VideoCapture(src)
        if not self.cap.isOpened():
            raise RuntimeError(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘æº: {src}")
        self.q: Queue[np.ndarray] = Queue(maxsize=queue_size)
        self.stopped = False

    def run(self):
        while not self.stopped:
            if not self.q.full():
                ret, frame = self.cap.read()
                if not ret:
                    self.stop()
                    break
                self.q.put(frame)
            else:
                time.sleep(0.001)  # è®©å‡º CPU

    def read(self, timeout: float = 1.0) -> Tuple[bool, np.ndarray | None]:
        try:
            frame = self.q.get(timeout=timeout)
            return True, frame
        except Empty:
            return False, None

    def more(self) -> bool:
        return not self.q.empty() or not self.stopped

    def stop(self):
        self.stopped = True
        self.cap.release()


# ----------------------------------------------------------------------
# æ¨¡å‹åŠ è½½
# ----------------------------------------------------------------------

def load_models(cfg: dict) -> tuple[YOLO, YOLO]:
    """åŠ è½½æ£€æµ‹ä¸åˆ†ç±»æ¨¡å‹ã€‚"""
    cfg["det_path"] = PROJECT_ROOT / f"runs/det/train/{cfg['det_run']}/weights/best.pt"
    cfg["cls_path"] = PROJECT_ROOT / f"runs/cls/train/{cfg['cls_run']}/weights/best.pt"

    for key in ("det_path", "cls_path"):
        if not cfg[key].exists():
            sys.exit(f"âŒ æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {cfg[key]}")

    print("ğŸ”¹ æ­£åœ¨åŠ è½½æ¨¡å‹ â€¦")
    det = YOLO(str(cfg["det_path"]))
    cls = YOLO(str(cfg["cls_path"]))

    device = cfg["device"]
    det.to(device)
    cls.to(device)

    if cfg["half"] and torch.cuda.is_available():
        det.model.half()
        cls.model.half()

    return det, cls


# ----------------------------------------------------------------------
# è§†é¢‘æºè§£æ
# ----------------------------------------------------------------------

def get_video_sources(src: Union[str, int, Path]) -> List[Union[int, str]]:
    """æ ¹æ®è¾“å…¥è¿”å›è§†é¢‘æºåˆ—è¡¨ã€‚"""
    if isinstance(src, int):
        return [src]
    src_path = Path(str(src))
    if src_path.exists():
        if src_path.is_file():
            return [str(src_path)]
        if src_path.is_dir():
            vids = sorted([p for p in src_path.iterdir() if p.suffix.lower() in {'.mp4', '.avi', '.mov', '.mkv'}])
            if not vids:
                sys.exit(f"âš ï¸ ç›®å½•ä¸ºç©º: {src_path}")
            return [str(v) for v in vids]
    # å…¶ä½™æƒ…å†µæŒ‰ URL/æœªçŸ¥è·¯å¾„å¤„ç†
    return [str(src)]


# ----------------------------------------------------------------------
# ç»˜åˆ¶è¾…åŠ©
# ----------------------------------------------------------------------

def draw_label(img: np.ndarray, text: str, tl: Tuple[int, int], color: Tuple[int, int, int]) -> None:
    """åœ¨ç›®æ ‡æ¡†å·¦ä¸Šç»˜åˆ¶å¸¦èƒŒæ™¯çš„æ–‡æœ¬ã€‚"""
    font_scale = max(img.shape[1] / 800.0, 0.6)
    thickness = max(1, int(font_scale * 2))
    (w, h), _ = cv2.getTextSize(text, CONFIG["font"], font_scale, thickness)
    top_left = tl
    bottom_right = (tl[0] + w + 10, tl[1] - h - 10)
    cv2.rectangle(img, top_left, bottom_right, color, -1, cv2.LINE_AA)
    cv2.putText(img, text, (tl[0] + 5, tl[1] - 5), CONFIG["font"], font_scale, (0, 0, 0), thickness + 1, cv2.LINE_AA)


# ----------------------------------------------------------------------
# è§†é¢‘å†™å…¥
# ----------------------------------------------------------------------

def unique_path(path: Path) -> Path:
    """è‹¥æ–‡ä»¶å·²å­˜åœ¨åˆ™è‡ªåŠ¨è¿½åŠ ç´¢å¼•é¿å…è¦†ç›–ã€‚"""
    if CONFIG["overwrite_output"] or not path.exists():
        return path
    stem, suf = path.stem, path.suffix
    for i in range(1, 1000):
        cand = path.with_name(f"{stem}_{i}{suf}")
        if not cand.exists():
            return cand
    return path


def create_writer(cfg: dict, source: Union[str, int], frame: np.ndarray) -> cv2.VideoWriter | None:
    """æ ¹æ®é¦–å¸§åˆ†è¾¨ç‡åˆ›å»º VideoWriterã€‚"""
    if not cfg["save_video"]:
        return None
    out_dir = SCRIPT_DIR / cfg["output_dir"]
    out_dir.mkdir(parents=True, exist_ok=True)

    name = f"camera_{source}.mp4" if isinstance(source, int) else f"{Path(str(source)).stem}.mp4"
    out_path = unique_path(out_dir / name)
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    h, w = frame.shape[:2]
    writer = cv2.VideoWriter(str(out_path), fourcc, cfg["target_fps"], (w, h))
    return writer


# ----------------------------------------------------------------------
# æ¨ç†ä¸»å¾ªç¯
# ----------------------------------------------------------------------

def process_stream(cfg: dict, source: Union[int, str], det: YOLO, cls: YOLO) -> None:
    grabber = FrameGrabber(source, queue_size=cfg["queue_size"])
    grabber.start()

    # åˆå§‹åŒ–è®¡æ—¶
    prev_time = time.time()
    avg_fps = 0.0
    frame_count = 0

    # ç­‰å¾…é¦–å¸§ä»¥ç¡®å®šåˆ†è¾¨ç‡
    ok, frame = grabber.read()
    if not ok:
        print("âš ï¸ æ— æ³•è¯»å–é¦–å¸§ï¼Œè·³è¿‡è¯¥æºã€‚")
        grabber.stop()
        return

    writer = create_writer(cfg, source, frame)

    pbar = tqdm(total=0, position=0, unit="fps", bar_format="{desc}")

    while grabber.more():
        ok, frame = grabber.read()
        if not ok:
            break

        frame_count += 1
        # é™æ€è·³å¸§
        if cfg["skip_frames"] > 1 and frame_count % cfg["skip_frames"] != 0:
            continue

        # ------------------------------------
        # 1. äººè„¸æ£€æµ‹
        # ------------------------------------
        det_res = det.predict(frame, conf=cfg["conf_thres"], device=cfg["device"], verbose=False)[0]
        boxes = det_res.boxes.xyxy.cpu().numpy() if det_res.boxes is not None else []

        faces = []
        coords = []
        for box in boxes:
            x1, y1, x2, y2 = map(int, box[:4])
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(frame.shape[1] - 1, x2), min(frame.shape[0] - 1, y2)
            face = frame[y1:y2, x1:x2]
            if face.size == 0:
                continue
            faces.append(face)
            coords.append((x1, y1, x2, y2))

        # ------------------------------------
        # 2. æ‰¹é‡è¡¨æƒ…åˆ†ç±»
        # ------------------------------------
        labels = []
        if faces:
            batch = []
            for f in faces:
                img = cv2.resize(f, (cfg["img_size"], cfg["img_size"]))
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                img = img.transpose(2, 0, 1) / 255.0
                batch.append(img)
            batch_tensor = torch.from_numpy(np.stack(batch)).float()
            if cfg["half"] and torch.cuda.is_available():
                batch_tensor = batch_tensor.half()
            batch_tensor = batch_tensor.to(cfg["device"])

            cls_res = cls.predict(batch_tensor, device=cfg["device"], verbose=False)
            for res in cls_res:
                prob = res.probs
                if prob is None:
                    labels.append("?")
                    continue
                idx = int(prob.top1)[0]
                label = cfg["names"][idx]
                labels.append(label)

        # ------------------------------------
        # 3. ç»˜åˆ¶ç»“æœ
        # ------------------------------------
        for (x1, y1, x2, y2), label in zip(coords, labels):
            idx = cfg["names"].index(label) if label in cfg["names"] else 0
            color = cfg["colors"][idx % len(cfg["colors"])]
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            draw_label(frame, label, (x1, y1), color)

        # ------------------------------------
        # 4. FPS & è¾“å‡º
        # ------------------------------------
        now = time.time()
        dt = now - prev_time
        prev_time = now
        fps = 1.0 / dt if dt > 0 else 0.0
        avg_fps = 0.9 * avg_fps + 0.1 * fps if avg_fps else fps
        pbar.set_description(f"{source} | FPS: {avg_fps:.1f}")

        if writer:
            writer.write(frame)
        if cfg["display"]:
            cv2.imshow(str(source), frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

        # åŠ¨æ€è°ƒèŠ‚è·³å¸§
        if cfg["auto_skip"] and avg_fps:
            if avg_fps < cfg["target_fps"] * 0.9:
                cfg["skip_frames"] = min(cfg["skip_frames"] + 1, 5)
            elif avg_fps > cfg["target_fps"] * 1.1:
                cfg["skip_frames"] = max(cfg["skip_frames"] - 1, 1)

    # æ¸…ç†
    grabber.stop()
    if writer:
        writer.release()
    cv2.destroyAllWindows()
    pbar.close()


# ----------------------------------------------------------------------
# é«˜å±‚ API
# ----------------------------------------------------------------------

def run_infer(*, input_source: Union[int, str] | None = None, target_fps: int | None = None) -> None:
    """ä¾›å¤–éƒ¨è„šæœ¬è°ƒç”¨çš„ç®€æ˜“å…¥å£ã€‚"""
    if input_source is not None:
        CONFIG["input"] = input_source
    if target_fps is not None:
        CONFIG["target_fps"] = target_fps

    det_model, cls_model = load_models(CONFIG)
    sources = get_video_sources(CONFIG["input"])
    for src in sources:
        process_stream(CONFIG, src, det_model, cls_model)


# ----------------------------------------------------------------------
# CLI å…¥å£
# ----------------------------------------------------------------------

if __name__ == "__main__":
    run_infer()
