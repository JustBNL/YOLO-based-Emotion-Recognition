import gradio as gr
import cv2
import numpy as np
from pathlib import Path
import tempfile
import time
import threading
from queue import Queue, Empty
from typing import Optional, List, Union
import torch

# å¯¼å…¥æ¨ç†æ¨¡å—
try:
    from infer_images import load_models as load_image_models, CONFIG as IMAGE_CONFIG
    from infer_stream import load_models as load_stream_models, CONFIG as STREAM_CONFIG, FrameGrabber
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨ç†æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿ infer_images.py å’Œ infer_stream.py åœ¨åŒä¸€ç›®å½•ä¸‹")
    exit(1)


class SimpleEmotionApp:
    def __init__(self):
        self.models_loaded = False
        self.det_model = None
        self.cls_model = None
        self.camera_active = False
        self.camera_thread = None
        self.frame_queue = Queue(maxsize=5)
        self.current_frame = None

        # ğŸš€ è·³å¸§ä¼˜åŒ–å‚æ•°
        self.frame_skip = 3  # æ¯3å¸§å¤„ç†1å¸§
        self.frame_counter = 0
        self.last_detection_result = None  # ç¼“å­˜ä¸Šæ¬¡æ£€æµ‹ç»“æœ
        self.detection_cache_frames = 5  # æ£€æµ‹ç»“æœç¼“å­˜å¸§æ•°
        self.cache_counter = 0

        # æ€§èƒ½ç»Ÿè®¡
        self.fps_counter = 0
        self.fps_start_time = time.time()
        self.current_fps = 0

    def load_models_once(self):
        """å»¶è¿ŸåŠ è½½æ¨¡å‹ï¼ŒåªåŠ è½½ä¸€æ¬¡"""
        if not self.models_loaded:
            print("ğŸ”¹ æ­£åœ¨åŠ è½½æ¨¡å‹...")
            # ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹é…ç½®
            self.det_model, self.cls_model = load_image_models(IMAGE_CONFIG)
            self.models_loaded = True
            print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    def draw_label(self, img: np.ndarray, text: str, tl: tuple, color: tuple):
        """ç»˜åˆ¶å¸¦èƒŒæ™¯çš„æ ‡ç­¾"""
        font_scale = max(img.shape[1] / 800.0, 0.8)
        thickness = max(1, int(font_scale * 2))
        font = cv2.FONT_HERSHEY_SIMPLEX

        (w, h), _ = cv2.getTextSize(text, font, font_scale, thickness)
        top_left = tl
        bottom_right = (tl[0] + w + 10, tl[1] - h - 10)

        cv2.rectangle(img, top_left, bottom_right, color, -1, cv2.LINE_AA)
        cv2.putText(img, text, (tl[0] + 5, tl[1] - 5), font, font_scale, (0, 0, 0), thickness + 1, cv2.LINE_AA)

    def process_image(self, image):
        """å¤„ç†å•å¼ å›¾ç‰‡ - å®ç°ä¸ infer_images.py ç›¸åŒçš„æ•ˆæœ"""
        if image is None:
            return None

        try:
            self.load_models_once()
            img = image.copy()

            # äººè„¸æ£€æµ‹
            det_res = self.det_model(img, imgsz=640, conf=IMAGE_CONFIG["conf_thres"],
                                     device=IMAGE_CONFIG["device"], verbose=False)[0]

            # å¤„ç†æ¯ä¸ªæ£€æµ‹åˆ°çš„äººè„¸
            for box in det_res.boxes.xyxy.cpu().numpy():
                x1, y1, x2, y2 = map(int, box)
                face = img[y1:y2, x1:x2]
                if face.size == 0:
                    continue

                # è°ƒæ•´äººè„¸å°ºå¯¸ç”¨äºåˆ†ç±»
                face_resized = cv2.resize(face, (IMAGE_CONFIG["img_size"], IMAGE_CONFIG["img_size"]))

                # è¡¨æƒ…åˆ†ç±»
                cls_res = self.cls_model(face_resized, imgsz=IMAGE_CONFIG["img_size"],
                                         device=IMAGE_CONFIG["device"], verbose=False)[0]

                class_bias = np.array([1, 1, 1, 1, 0.5, 1, 1], dtype=np.float32)
                logits = cls_res.probs.data.cpu().numpy()
                adjusted_logits = logits * class_bias
                probs = adjusted_logits / np.sum(adjusted_logits)

                idx = int(np.argmax(probs))
                prob = float(probs[idx])

                # ç”Ÿæˆæ ‡ç­¾
                if IMAGE_CONFIG.get("label_display") == "code":
                    label = f"{idx}"
                else:
                    label = f"{IMAGE_CONFIG['names'][idx]} {prob:.2f}"

                # ç»˜åˆ¶æ£€æµ‹æ¡†å’Œæ ‡ç­¾
                color = IMAGE_CONFIG["colors"][idx % len(IMAGE_CONFIG["colors"])]
                cv2.rectangle(img, (x1, y1), (x2, y2), color, 2, cv2.LINE_AA)
                self.draw_label(img, label, (x1, y1), color)

            return img

        except Exception as e:
            print(f"âŒ å›¾ç‰‡å¤„ç†å¤±è´¥: {e}")
            return image

    def process_images_batch(self, files):
        """æ‰¹é‡å¤„ç†å›¾ç‰‡ - ä¿®å¤bug1: è¿”å›æ­£ç¡®æ ¼å¼ç»™Gallery"""
        if not files:
            return []

        results = []
        for file in files:
            try:
                image = cv2.imread(file.name)
                if image is not None:
                    # è½¬æ¢ä¸ºRGB
                    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                    result = self.process_image(image_rgb)
                    if result is not None:
                        # Galleryéœ€è¦PIL Imageæˆ–æ–‡ä»¶è·¯å¾„ï¼Œæˆ‘ä»¬ä¿å­˜å¤„ç†åçš„å›¾ç‰‡
                        temp_path = f"temp_result_{len(results)}.jpg"
                        result_bgr = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)
                        cv2.imwrite(temp_path, result_bgr)
                        results.append(temp_path)
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file.name}: {e}")

        return results

    def process_video(self, video_file, progress=gr.Progress()):
        """å¤„ç†è§†é¢‘æ–‡ä»¶ - ä½¿ç”¨ä¸´æ—¶ç›®å½•ä¿å­˜"""
        if video_file is None:
            return None, "è¯·å…ˆä¸Šä¼ è§†é¢‘æ–‡ä»¶"

        try:
            self.load_models_once()

            # ä½¿ç”¨ä¸´æ—¶ç›®å½•
            output_dir = Path("temp_output")
            output_dir.mkdir(exist_ok=True)

            input_path = video_file.name
            input_name = Path(input_path).stem
            output_path = output_dir / f"processed_{input_name}.mp4"

            cap = cv2.VideoCapture(input_path)
            if not cap.isOpened():
                return None, "æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶"

            # è·å–è§†é¢‘å±æ€§
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

            # ä½¿ç”¨H.264ç¼–ç å™¨
            fourcc = cv2.VideoWriter_fourcc(*'H264')
            out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))

            frame_count = 0

            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                # ä½¿ç”¨ä¸æ‘„åƒå¤´ç›¸åŒçš„å¤„ç†é€»è¾‘
                det_res = self.det_model.predict(frame, conf=STREAM_CONFIG["conf_thres"],
                                                 device=STREAM_CONFIG["device"], verbose=False)[0]
                boxes = det_res.boxes.xyxy.cpu().numpy() if det_res.boxes is not None else []

                faces = []
                coords = []
                for box in boxes:
                    x1, y1, x2, y2 = map(int, box[:4])
                    x1, y1 = max(0, x1), max(0, y1)
                    x2, y2 = min(frame.shape[1] - 1, x2), min(frame.shape[0] - 1, y2)
                    if x2 <= x1 or y2 <= y1:
                        continue
                    face = frame[y1:y2, x1:x2]
                    if face.size == 0:
                        continue
                    faces.append(face)
                    coords.append((x1, y1, x2, y2))

                # æ‰¹é‡è¡¨æƒ…åˆ†ç±»
                if faces:
                    try:
                        batch = []
                        for f in faces:
                            img = cv2.resize(f, (STREAM_CONFIG["img_size"], STREAM_CONFIG["img_size"]))
                            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                            img = img.transpose(2, 0, 1) / 255.0
                            batch.append(img)

                        batch_tensor = torch.from_numpy(np.stack(batch)).float()
                        batch_tensor = batch_tensor.to(STREAM_CONFIG["device"])

                        cls_res = self.cls_model.predict(batch_tensor, device=STREAM_CONFIG["device"], verbose=False)

                        # ä½¿ç”¨ä¸æ‘„åƒå¤´ç›¸åŒçš„ç±»åˆ«æƒé‡
                        class_bias = torch.tensor([2.0, 1.7, 1.5, 0.9, 0.1, 7.0, 2.0], device=STREAM_CONFIG["device"])

                        # ç»˜åˆ¶ç»“æœ
                        for (x1, y1, x2, y2), res in zip(coords, cls_res):
                            if res.probs is None:
                                continue

                            logits = res.probs.data
                            if not isinstance(logits, torch.Tensor):
                                continue

                            # åŠ æƒå¹¶softmax
                            adjusted_logits = logits * class_bias
                            adjusted_probs = torch.nn.functional.softmax(adjusted_logits, dim=0)
                            idx = int(torch.argmax(adjusted_probs))
                            prob = float(adjusted_probs[idx])

                            label = f"{STREAM_CONFIG['names'][idx]} {prob:.2f}"
                            color = STREAM_CONFIG["colors"][idx % len(STREAM_CONFIG["colors"])]

                            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                            self.draw_label(frame, label, (x1, y1), color)

                    except Exception as e:
                        print(f"âš ï¸ åˆ†ç±»å¤±è´¥: {e}")

                out.write(frame)

                frame_count += 1
                if frame_count % 10 == 0:  # æ¯10å¸§æ›´æ–°ä¸€æ¬¡è¿›åº¦
                    progress_value = frame_count / total_frames
                    progress(progress_value, f"å¤„ç†è¿›åº¦: {frame_count}/{total_frames} å¸§")

            cap.release()
            out.release()

            return str(output_path), f"è§†é¢‘å¤„ç†å®Œæˆï¼å…±å¤„ç† {frame_count} å¸§"

        except Exception as e:
            print(f"âŒ è§†é¢‘å¤„ç†å¤±è´¥: {e}")
            return None, f"è§†é¢‘å¤„ç†å¤±è´¥: {str(e)}"

    def process_camera_frame_optimized(self, frame):
        """ğŸš€ ä¼˜åŒ–ç‰ˆæ‘„åƒå¤´å¸§å¤„ç† - ä½¿ç”¨è·³å¸§å’Œç»“æœç¼“å­˜"""
        try:
            self.frame_counter += 1

            # å†³å®šæ˜¯å¦éœ€è¦é‡æ–°æ£€æµ‹
            should_detect = (
                    self.frame_counter % self.frame_skip == 0 or  # è·³å¸§é—´éš”
                    self.last_detection_result is None or  # æ²¡æœ‰ç¼“å­˜ç»“æœ
                    self.cache_counter >= self.detection_cache_frames  # ç¼“å­˜è¿‡æœŸ
            )

            if should_detect:
                # æ‰§è¡Œå®Œæ•´çš„æ£€æµ‹å’Œåˆ†ç±»
                detection_result = self._full_detection_and_classification(frame)
                self.last_detection_result = detection_result
                self.cache_counter = 0
                print(f"ğŸ”„ å®Œæ•´æ£€æµ‹ - å¸§ {self.frame_counter}")
            else:
                # ä½¿ç”¨ç¼“å­˜ç»“æœï¼Œåªæ›´æ–°åæ ‡ï¼ˆå¯é€‰ï¼‰
                detection_result = self.last_detection_result
                self.cache_counter += 1
                print(f"âš¡ ä½¿ç”¨ç¼“å­˜ - å¸§ {self.frame_counter}")

            # ç»˜åˆ¶ç»“æœ
            processed_frame = self._draw_detection_results(frame, detection_result)

            # æ›´æ–°FPSç»Ÿè®¡
            self._update_fps_stats()

            return processed_frame

        except Exception as e:
            print(f"âŒ æ‘„åƒå¤´å¸§å¤„ç†å¤±è´¥: {e}")
            return frame

    def _full_detection_and_classification(self, frame):
        """æ‰§è¡Œå®Œæ•´çš„æ£€æµ‹å’Œåˆ†ç±»"""
        try:
            # 1. äººè„¸æ£€æµ‹
            det_res = self.det_model.predict(
                frame,
                conf=STREAM_CONFIG["conf_thres"],
                device=STREAM_CONFIG["device"],
                verbose=False
            )[0]

            boxes = det_res.boxes.xyxy.cpu().numpy() if det_res.boxes is not None else []

            # æ”¶é›†æ‰€æœ‰äººè„¸åŒºåŸŸ
            faces = []
            coords = []
            valid_detections = []

            for box in boxes:
                x1, y1, x2, y2 = map(int, box[:4])
                x1, y1 = max(0, x1), max(0, y1)
                x2, y2 = min(frame.shape[1] - 1, x2), min(frame.shape[0] - 1, y2)
                if x2 <= x1 or y2 <= y1:
                    continue
                face = frame[y1:y2, x1:x2]
                if face.size == 0:
                    continue
                faces.append(face)
                coords.append((x1, y1, x2, y2))

            # 2. æ‰¹é‡è¡¨æƒ…åˆ†ç±»
            labels = []
            if faces:
                try:
                    # æ‰¹é‡å‡†å¤‡æ‰€æœ‰å›¾åƒ
                    batch = []
                    for f in faces:
                        img = cv2.resize(f, (STREAM_CONFIG["img_size"], STREAM_CONFIG["img_size"]))
                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                        img = img.transpose(2, 0, 1) / 255.0
                        batch.append(img)

                    # è½¬æ¢ä¸ºå¼ é‡å¹¶æ‰¹é‡æ¨ç†
                    batch_tensor = torch.from_numpy(np.stack(batch)).float()
                    batch_tensor = batch_tensor.to(STREAM_CONFIG["device"])

                    # æ‰¹é‡å¤„ç†æ‰€æœ‰äººè„¸
                    cls_res = self.cls_model.predict(
                        batch_tensor,
                        device=STREAM_CONFIG["device"],
                        verbose=False
                    )

                    # ç±»åˆ«æƒé‡
                    class_bias = torch.tensor(
                        [2.0, 1.7, 1.5, 0.8, 0.1, 7.0, 2.0],
                        device=STREAM_CONFIG["device"]
                    )

                    # å¤„ç†æ¯ä¸ªç»“æœ
                    for res in cls_res:
                        if res.probs is None:
                            labels.append("?")
                            continue

                        logits = res.probs.data
                        if not isinstance(logits, torch.Tensor):
                            labels.append("?")
                            continue

                        # åº”ç”¨åŠ æƒå¹¶softmax
                        adjusted_logits = logits * class_bias
                        adjusted_probs = torch.nn.functional.softmax(adjusted_logits, dim=0)
                        idx = int(torch.argmax(adjusted_probs))
                        prob = float(adjusted_probs[idx])

                        # è·å–æ ‡ç­¾
                        if 0 <= idx < len(STREAM_CONFIG["names"]):
                            label = f"{STREAM_CONFIG['names'][idx]} {prob:.2f}"
                        else:
                            label = "unknown"
                        labels.append(label)

                except Exception as e:
                    print(f"âš ï¸ æ‰¹é‡åˆ†ç±»å¤±è´¥: {e}")
                    labels = ["error"] * len(faces)

            # è¿”å›æ£€æµ‹ç»“æœ
            return {
                'coords': coords,
                'labels': labels,
                'timestamp': time.time()
            }

        except Exception as e:
            print(f"âŒ å®Œæ•´æ£€æµ‹å¤±è´¥: {e}")
            return {'coords': [], 'labels': [], 'timestamp': time.time()}

    def _draw_detection_results(self, frame, detection_result):
        """åœ¨å¸§ä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ"""
        if not detection_result or not detection_result['coords']:
            return frame

        try:
            coords = detection_result['coords']
            labels = detection_result['labels']

            # ç»˜åˆ¶ç»“æœ
            for (x1, y1, x2, y2), label in zip(coords, labels):
                try:
                    # ä»æ ‡ç­¾ä¸­æå–æƒ…æ„Ÿåç§°
                    emotion_name = label.split()[0] if ' ' in label else label

                    # æŸ¥æ‰¾å¯¹åº”çš„é¢œè‰²
                    if emotion_name in STREAM_CONFIG["names"]:
                        idx = STREAM_CONFIG["names"].index(emotion_name)
                    else:
                        idx = 0  # é»˜è®¤ä¸ºç¬¬ä¸€ä¸ªé¢œè‰²

                    color = STREAM_CONFIG["colors"][idx % len(STREAM_CONFIG["colors"])]

                    # ç»˜åˆ¶æ£€æµ‹æ¡†
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)

                    # ç»˜åˆ¶æ ‡ç­¾
                    self.draw_label(frame, label, (x1, y1), color)

                except Exception as e:
                    print(f"âš ï¸ ç»˜åˆ¶å¤±è´¥: {e}")

            # æ·»åŠ æ€§èƒ½ä¿¡æ¯æ˜¾ç¤º
            self._draw_performance_info(frame)

            return frame

        except Exception as e:
            print(f"âŒ ç»˜åˆ¶æ£€æµ‹ç»“æœå¤±è´¥: {e}")
            return frame

    def _draw_performance_info(self, frame):
        """åœ¨å¸§ä¸Šç»˜åˆ¶æ€§èƒ½ä¿¡æ¯"""
        try:
            # æ€§èƒ½ä¿¡æ¯
            perf_info = [
                f"FPS: {self.current_fps:.1f}",
                f"Skip: 1/{self.frame_skip}",
                f"Cache: {self.cache_counter}/{self.detection_cache_frames}"
            ]

            # ç»˜åˆ¶åŠé€æ˜èƒŒæ™¯
            overlay = frame.copy()
            cv2.rectangle(overlay, (10, 10), (300, 80), (0, 0, 0), -1)
            cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)

            # ç»˜åˆ¶æ–‡æœ¬
            for i, info in enumerate(perf_info):
                y = 30 + i * 20
                cv2.putText(frame, info, (20, y), cv2.FONT_HERSHEY_SIMPLEX,
                            0.5, (0, 255, 0), 1, cv2.LINE_AA)

        except Exception as e:
            print(f"âš ï¸ ç»˜åˆ¶æ€§èƒ½ä¿¡æ¯å¤±è´¥: {e}")

    def _update_fps_stats(self):
        """æ›´æ–°FPSç»Ÿè®¡"""
        self.fps_counter += 1
        current_time = time.time()
        elapsed = current_time - self.fps_start_time

        if elapsed >= 1.0:  # æ¯ç§’æ›´æ–°ä¸€æ¬¡FPS
            self.current_fps = self.fps_counter / elapsed
            self.fps_counter = 0
            self.fps_start_time = current_time

    def get_available_cameras(self):
        """è·å–å¯ç”¨æ‘„åƒå¤´"""
        cameras = []
        for i in range(5):
            cap = cv2.VideoCapture(i)
            if cap.isOpened():
                ret, _ = cap.read()
                if ret:
                    cameras.append(f"æ‘„åƒå¤´ {i}")
                cap.release()
        return cameras if cameras else ["æ— å¯ç”¨æ‘„åƒå¤´"]

    def start_camera(self, camera_choice, skip_frames=3):
        """å¯åŠ¨æ‘„åƒå¤´ - æ”¯æŒè‡ªå®šä¹‰è·³å¸§è®¾ç½®"""
        if camera_choice == "æ— å¯ç”¨æ‘„åƒå¤´":
            return "æ²¡æœ‰å¯ç”¨çš„æ‘„åƒå¤´"

        # åœæ­¢ç°æœ‰æ‘„åƒå¤´
        if self.camera_active:
            self.stop_camera()
            time.sleep(0.5)

        try:
            # è§£ææ‘„åƒå¤´ID
            camera_id = int(camera_choice.split()[-1])

            # è®¾ç½®è·³å¸§å‚æ•°
            self.frame_skip = max(1, skip_frames)
            self.frame_counter = 0
            self.last_detection_result = None
            self.cache_counter = 0

            self.load_models_once()
            self.camera_active = True

            def camera_worker():
                try:
                    grabber = FrameGrabber(camera_id, queue_size=3)
                    grabber.start()

                    while self.camera_active:
                        ok, frame = grabber.read(timeout=1.0)
                        if not ok:
                            continue

                        # ğŸš€ ä½¿ç”¨ä¼˜åŒ–ç‰ˆçš„å¸§å¤„ç†
                        processed_frame = self.process_camera_frame_optimized(frame)

                        # è½¬æ¢ä¸ºRGBå¹¶æ›´æ–°å½“å‰å¸§
                        if processed_frame is not None:
                            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
                            self.current_frame = frame_rgb

                    grabber.stop()

                except Exception as e:
                    print(f"æ‘„åƒå¤´å·¥ä½œçº¿ç¨‹é”™è¯¯: {e}")
                    self.camera_active = False

            self.camera_thread = threading.Thread(target=camera_worker, daemon=True)
            self.camera_thread.start()

            return f"æ‘„åƒå¤´å·²å¯åŠ¨: {camera_choice} (è·³å¸§: 1/{self.frame_skip})"

        except Exception as e:
            self.camera_active = False
            return f"å¯åŠ¨æ‘„åƒå¤´å¤±è´¥: {e}"

    def stop_camera(self):
        """åœæ­¢æ‘„åƒå¤´"""
        self.camera_active = False
        if self.camera_thread:
            self.camera_thread.join(timeout=2.0)
        self.current_frame = None

        # é‡ç½®ä¼˜åŒ–å‚æ•°
        self.frame_counter = 0
        self.last_detection_result = None
        self.cache_counter = 0
        self.fps_counter = 0
        self.fps_start_time = time.time()

        return "æ‘„åƒå¤´å·²åœæ­¢"

    def get_camera_frame(self):
        """è·å–æ‘„åƒå¤´å¸§"""
        return self.current_frame if self.camera_active else None

    def update_skip_frames(self, skip_frames):
        """åŠ¨æ€æ›´æ–°è·³å¸§è®¾ç½®"""
        self.frame_skip = max(1, skip_frames)
        return f"è·³å¸§è®¾ç½®å·²æ›´æ–°: 1/{self.frame_skip}"


# åˆ›å»ºåº”ç”¨å®ä¾‹
app = SimpleEmotionApp()


def create_interface():
    """åˆ›å»ºç®€åŒ–çš„Gradioç•Œé¢"""

    # åˆ›å»ºç•Œé¢
    with gr.Blocks(title="æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿ", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# ğŸ­ æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿ (æ€§èƒ½ä¼˜åŒ–ç‰ˆ)")
        gr.Markdown("åŸºäº YOLO çš„å®æ—¶æƒ…æ„Ÿè¯†åˆ« - æ”¯æŒè·³å¸§ä¼˜åŒ–")

        with gr.Tabs():
            # å›¾ç‰‡è¯†åˆ«
            with gr.Tab("ğŸ“¸ å›¾ç‰‡è¯†åˆ«"):
                with gr.Row():
                    with gr.Column():
                        image_input = gr.File(
                            label="ä¸Šä¼ å›¾ç‰‡",
                            file_types=["image"],
                            file_count="multiple"
                        )
                        image_btn = gr.Button("å¼€å§‹è¯†åˆ«", variant="primary")

                        # æ·»åŠ æ“ä½œæç¤º
                        gr.Markdown("æç¤ºï¼šä½¿ç”¨é¼ æ ‡æ»šè½®æˆ–æ‹–åŠ¨æ»šåŠ¨æ¡æŸ¥çœ‹æ‰€æœ‰å›¾ç‰‡")

                    with gr.Column():
                        # ä¿®å¤bug1: ç§»é™¤ rows å±æ€§å¹¶å¢åŠ é«˜åº¦
                        image_output = gr.Gallery(
                            label="è¯†åˆ«ç»“æœ",
                            columns=2,  # ä¿æŒ2åˆ—å¸ƒå±€
                            height=500,  # è®¾ç½®å›ºå®šé«˜åº¦
                            allow_preview=True,
                            show_download_button=True,
                            scroll_to_output=True  # æ·»åŠ æ­¤å±æ€§ç¡®ä¿ç»“æœå¯è§
                        )

                image_btn.click(
                    fn=app.process_images_batch,
                    inputs=image_input,
                    outputs=image_output
                )

            # è§†é¢‘è¯†åˆ«
            with gr.Tab("ğŸ¬ è§†é¢‘è¯†åˆ«"):
                with gr.Row():
                    with gr.Column():
                        video_input = gr.File(
                            label="ä¸Šä¼ è§†é¢‘",
                            file_types=["video"]
                        )
                        video_btn = gr.Button("å¼€å§‹å¤„ç†", variant="primary")

                    with gr.Column():
                        video_status = gr.Textbox(
                            label="å¤„ç†çŠ¶æ€",
                            value="ç­‰å¾…ä¸Šä¼ è§†é¢‘...",
                            interactive=False
                        )
                        video_output = gr.Video(
                            label="å¤„ç†ç»“æœ",
                            height=400
                        )

                video_btn.click(
                    fn=app.process_video,
                    inputs=video_input,
                    outputs=[video_output, video_status]
                )

            # ğŸš€ å®æ—¶æ‘„åƒå¤´ (ä¼˜åŒ–ç‰ˆ)
            with gr.Tab("ğŸ“¹ å®æ—¶æ‘„åƒå¤´"):
                with gr.Row():
                    with gr.Column(scale=1):
                        camera_dropdown = gr.Dropdown(
                            choices=app.get_available_cameras(),
                            value=app.get_available_cameras()[0],
                            label="é€‰æ‹©æ‘„åƒå¤´"
                        )

                        # ğŸš€ è·³å¸§è®¾ç½®
                        skip_frames_slider = gr.Slider(
                            minimum=1,
                            maximum=10,
                            value=3,
                            step=1,
                            label="è·³å¸§è®¾ç½® (1=æ— è·³å¸§, 3=æ¯3å¸§å¤„ç†1å¸§)",
                            info="æ•°å€¼è¶Šå¤§æ€§èƒ½è¶Šå¥½ï¼Œä½†å“åº”ç¨æ…¢"
                        )

                        start_btn = gr.Button("å¯åŠ¨æ‘„åƒå¤´", variant="primary")
                        stop_btn = gr.Button("åœæ­¢æ‘„åƒå¤´", variant="secondary")

                        update_skip_btn = gr.Button("æ›´æ–°è·³å¸§è®¾ç½®", variant="secondary")

                        camera_status = gr.Textbox(
                            label="çŠ¶æ€",
                            value="æ‘„åƒå¤´æœªå¯åŠ¨",
                            interactive=False
                        )

                    with gr.Column(scale=2):
                        camera_output = gr.Image(
                            label="å®æ—¶ç”»é¢",
                            height=500
                        )

                # æ›´æ–°æ‘„åƒå¤´ç”»é¢
                def update_camera():
                    frame = app.get_camera_frame()
                    return frame

                # å¯åŠ¨æ‘„åƒå¤´ï¼ˆå¸¦è·³å¸§è®¾ç½®ï¼‰
                def start_camera_with_skip(camera_choice, skip_frames):
                    return app.start_camera(camera_choice, skip_frames)

                # å¯åŠ¨æ‘„åƒå¤´
                start_btn.click(
                    fn=start_camera_with_skip,
                    inputs=[camera_dropdown, skip_frames_slider],
                    outputs=camera_status
                )

                # åœæ­¢æ‘„åƒå¤´
                stop_btn.click(
                    fn=app.stop_camera,
                    outputs=camera_status
                )

                # åŠ¨æ€æ›´æ–°è·³å¸§è®¾ç½®
                update_skip_btn.click(
                    fn=app.update_skip_frames,
                    inputs=skip_frames_slider,
                    outputs=camera_status
                )

                # åˆ›å»ºå®šæ—¶å™¨æ¥æ›´æ–°æ‘„åƒå¤´ç”»é¢
                camera_timer = gr.Timer(value=0.1)  # æ¯100msæ›´æ–°ä¸€æ¬¡
                camera_timer.tick(
                    fn=update_camera,
                    outputs=camera_output
                )

        # ä½¿ç”¨è¯´æ˜
        with gr.Accordion("ä½¿ç”¨è¯´æ˜", open=False):
            gr.Markdown("""
            ### ğŸš€ æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½
            - **è·³å¸§å¤„ç†**: å¯è®¾ç½®æ¯Nå¸§å¤„ç†ä¸€æ¬¡ï¼Œå¤§å¹…æå‡æ€§èƒ½
            - **ç»“æœç¼“å­˜**: éå¤„ç†å¸§ä½¿ç”¨ä¸Šæ¬¡æ£€æµ‹ç»“æœï¼Œä¿æŒæµç•…åº¦
            - **å®æ—¶FPSæ˜¾ç¤º**: ç•Œé¢æ˜¾ç¤ºå½“å‰å¸§ç‡å’Œä¼˜åŒ–çŠ¶æ€
            - **åŠ¨æ€è°ƒèŠ‚**: å¯åœ¨è¿è¡Œæ—¶è°ƒæ•´è·³å¸§è®¾ç½®

            ### æ”¯æŒçš„æƒ…æ„Ÿç±»åˆ«
            - ğŸ˜  ç”Ÿæ°” (Angry)
            - ğŸ¤¢ åŒæ¶ (Disgust)
            - ğŸ˜¨ ææƒ§ (Fear)
            - ğŸ˜Š å¿«ä¹ (Happy)
            - ğŸ˜¢ æ‚²ä¼¤ (Sad)
            - ğŸ˜ ä¸­æ€§ (Neutral)
            - ğŸ˜² æƒŠè®¶ (Surprise)

            ### ä½¿ç”¨æ–¹æ³•
            1. **å›¾ç‰‡è¯†åˆ«**: ä¸Šä¼ å•å¼ æˆ–å¤šå¼ å›¾ç‰‡è¿›è¡Œæ‰¹é‡å¤„ç†
            2. **è§†é¢‘è¯†åˆ«**: ä¸Šä¼ è§†é¢‘æ–‡ä»¶ï¼Œç³»ç»Ÿä¼šé€å¸§å¤„ç†å¹¶è¾“å‡ºç»“æœè§†é¢‘
            3. **å®æ—¶æ‘„åƒå¤´**: é€‰æ‹©æ‘„åƒå¤´è¿›è¡Œå®æ—¶æƒ…æ„Ÿè¯†åˆ«
               - è°ƒèŠ‚è·³å¸§è®¾ç½®ä»¥å¹³è¡¡æ€§èƒ½å’Œå“åº”é€Ÿåº¦
               - æŸ¥çœ‹å®æ—¶FPSå’Œä¼˜åŒ–çŠ¶æ€

            ### è·³å¸§è®¾ç½®å»ºè®®
            - **è·³å¸§=1**: æ— è·³å¸§ï¼Œæœ€é«˜ç²¾åº¦ï¼Œé€‚åˆé«˜æ€§èƒ½è®¾å¤‡
                - **è·³å¸§=3**: æ¨èè®¾ç½®ï¼Œå¹³è¡¡æ€§èƒ½å’Œç²¾åº¦
                - **è·³å¸§=5**: é«˜æ€§èƒ½æ¨¡å¼ï¼Œé€‚åˆä½é…ç½®è®¾å¤‡
                - **è·³å¸§=10**: æé™æ€§èƒ½æ¨¡å¼ï¼Œå“åº”è¾ƒæ…¢ä½†æµç•…

                ### æ€§èƒ½ç›‘æ§è¯´æ˜
                - **FPS**: å½“å‰å¤„ç†å¸§ç‡
                - **Skip**: å½“å‰è·³å¸§è®¾ç½®
                - **Cache**: ç¼“å­˜ä½¿ç”¨æƒ…å†µ

                ### æ³¨æ„äº‹é¡¹
                - ç¡®ä¿å›¾ç‰‡ä¸­åŒ…å«æ¸…æ™°çš„äººè„¸
                - è§†é¢‘å¤„ç†å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…
                - æ‘„åƒå¤´åŠŸèƒ½éœ€è¦è®¾å¤‡æ”¯æŒ
                - è·³å¸§è®¾ç½®è¿‡é«˜å¯èƒ½å¯¼è‡´å¿«é€ŸåŠ¨ä½œè¯†åˆ«å»¶è¿Ÿ

                ### å·²ä¼˜åŒ–çš„åŠŸèƒ½
                - âœ… å›¾ç‰‡å¤„ç†ç»“æœç°åœ¨å¯ä»¥æ­£å¸¸æ»šåŠ¨æŸ¥çœ‹
                - âœ… è§†é¢‘å¤„ç†å®Œæˆåèƒ½æ­£ç¡®æ˜¾ç¤ºç»“æœ
                - âœ… æ‘„åƒå¤´ç”»é¢ç°åœ¨èƒ½å®æ—¶æ˜¾ç¤º
                - ğŸš€ æ‘„åƒå¤´æµç°åœ¨æ”¯æŒè·³å¸§ä¼˜åŒ–ï¼Œå¤§å¹…æå‡æ€§èƒ½
                - ğŸš€ æ·»åŠ äº†å®æ—¶æ€§èƒ½ç›‘æ§å’ŒåŠ¨æ€å‚æ•°è°ƒèŠ‚
                """)

    return interface

if __name__ == "__main__":
    interface = create_interface()
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=True
    )
