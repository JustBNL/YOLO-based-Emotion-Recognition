import gradio as gr
import cv2
import numpy as np
from pathlib import Path
import time
import threading
from queue import Queue
from typing import Optional, List
import torch

# å¯¼å…¥æ¨ç†æ¨¡å—
try:
    from infer_images import load_models as load_image_models, CONFIG as IMAGE_CONFIG
    from infer_stream import load_models as load_stream_models, CONFIG as STREAM_CONFIG, FrameGrabber
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨ç†æ¨¡å—å¤±è´¥: {e}")
    exit(1)


class OptimizedEmotionApp:
    def __init__(self):
        self.models_loaded = False
        self.det_model = None
        self.cls_model = None
        self.camera_active = False
        self.camera_thread = None
        self.current_frame = None

        # æ€§èƒ½ä¼˜åŒ–å‚æ•°
        self.frame_skip = 3
        self.frame_counter = 0
        self.last_detection_result = None
        self.cache_counter = 0
        self.cache_frames = 5

        # FPSç»Ÿè®¡
        self.fps_counter = 0
        self.fps_start_time = time.time()
        self.current_fps = 0

    def load_models_once(self):
        """å»¶è¿ŸåŠ è½½æ¨¡å‹"""
        if not self.models_loaded:
            print("ğŸ”¹ æ­£åœ¨åŠ è½½æ¨¡å‹...")
            self.det_model, self.cls_model = load_image_models(IMAGE_CONFIG)
            self.models_loaded = True
            print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    def draw_label(self, img: np.ndarray, text: str, pos: tuple, color: tuple):
        """ç»˜åˆ¶æ ‡ç­¾"""
        font_scale = max(img.shape[1] / 800.0, 0.8)
        thickness = max(1, int(font_scale * 2))

        (w, h), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)
        cv2.rectangle(img, pos, (pos[0] + w + 10, pos[1] - h - 10), color, -1)
        cv2.putText(img, text, (pos[0] + 5, pos[1] - 5), cv2.FONT_HERSHEY_SIMPLEX,
                    font_scale, (0, 0, 0), thickness + 1)

    def process_detection(self, img, config, use_batch=False):
        """ç»Ÿä¸€çš„æ£€æµ‹å¤„ç†å‡½æ•°"""
        # äººè„¸æ£€æµ‹
        det_res = self.det_model(img, imgsz=640, conf=config["conf_thres"],
                                 device=config["device"], verbose=False)[0]

        results = []
        faces = []
        coords = []

        # æ”¶é›†äººè„¸
        for box in det_res.boxes.xyxy.cpu().numpy():
            x1, y1, x2, y2 = map(int, box)
            if x2 <= x1 or y2 <= y1:
                continue
            face = img[y1:y2, x1:x2]
            if face.size == 0:
                continue
            faces.append(face)
            coords.append((x1, y1, x2, y2))

        if not faces:
            return img

        # è¡¨æƒ…åˆ†ç±»
        if use_batch and len(faces) > 1:
            # æ‰¹é‡å¤„ç†
            batch = []
            for face in faces:
                face_resized = cv2.resize(face, (config["img_size"], config["img_size"]))
                face_rgb = cv2.cvtColor(face_resized, cv2.COLOR_BGR2RGB)
                batch.append(face_rgb.transpose(2, 0, 1) / 255.0)

            batch_tensor = torch.from_numpy(np.stack(batch)).float().to(config["device"])
            cls_results = self.cls_model.predict(batch_tensor, device=config["device"], verbose=False)

            # å¤„ç†æ‰¹é‡ç»“æœ
            for i, (cls_res, (x1, y1, x2, y2)) in enumerate(zip(cls_results, coords)):
                label, color = self._process_classification_result(cls_res, config)
                cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
                self.draw_label(img, label, (x1, y1), color)
        else:
            # å•ä¸ªå¤„ç†
            for face, (x1, y1, x2, y2) in zip(faces, coords):
                face_resized = cv2.resize(face, (config["img_size"], config["img_size"]))
                cls_res = self.cls_model(face_resized, imgsz=config["img_size"],
                                         device=config["device"], verbose=False)[0]

                label, color = self._process_classification_result(cls_res, config)
                cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
                self.draw_label(img, label, (x1, y1), color)

        return img

    def _process_classification_result(self, cls_res, config):
        """å¤„ç†åˆ†ç±»ç»“æœ"""
        if hasattr(cls_res, 'probs') and cls_res.probs is not None:
            if config == IMAGE_CONFIG:
                # å›¾ç‰‡æ¨¡å¼
                class_bias = np.array([1, 1, 1, 1, 1, 1, 1], dtype=np.float32)
                logits = cls_res.probs.data.cpu().numpy()
                adjusted_logits = logits * class_bias
                probs = adjusted_logits / np.sum(adjusted_logits)
                idx = int(np.argmax(probs))
                prob = float(probs[idx])
            else:
                # è§†é¢‘/æ‘„åƒå¤´æ¨¡å¼
                class_bias = torch.tensor([1.4, 1.7, 1.5, 0.9, 1.3, 0.7, 1.6], device=config["device"])
                logits = cls_res.probs.data
                adjusted_logits = logits * class_bias
                adjusted_probs = torch.nn.functional.softmax(adjusted_logits, dim=0)
                idx = int(torch.argmax(adjusted_probs))
                prob = float(adjusted_probs[idx])
        else:
            idx, prob = 0, 0.0

        # ç”Ÿæˆæ ‡ç­¾
        if config.get("label_display") == "code":
            label = f"{idx}"
        else:
            label = f"{config['names'][idx]} {prob:.2f}"

        color = config["colors"][idx % len(config["colors"])]
        return label, color

    def process_image(self, image):
        """å¤„ç†å•å¼ å›¾ç‰‡"""
        if image is None:
            return None
        try:
            self.load_models_once()
            return self.process_detection(image.copy(), IMAGE_CONFIG)
        except Exception as e:
            print(f"âŒ å›¾ç‰‡å¤„ç†å¤±è´¥: {e}")
            return image

    def process_images_batch(self, files):
        """æ‰¹é‡å¤„ç†å›¾ç‰‡"""
        if not files:
            return None

        results = []
        temp_dir = Path("temp_output/images")  # ä¿®æ”¹ä¸ºç»Ÿä¸€çš„ä¸´æ—¶ç›®å½•
        temp_dir.mkdir(parents=True, exist_ok=True)

        # æ¸…ç†æ—§çš„ä¸´æ—¶æ–‡ä»¶
        for old_file in temp_dir.glob("*.jpg"):
            try:
                old_file.unlink()
            except:
                pass

        for i, file in enumerate(files):
            try:
                image = cv2.imread(file.name)
                if image is not None:
                    result = self.process_image(image)  # ç›´æ¥å¤„ç†BGRæ ¼å¼
                    if result is not None:
                        # ä½¿ç”¨æ›´è§„èŒƒçš„æ–‡ä»¶å‘½å
                        temp_path = temp_dir / f"result_{i:03d}.jpg"
                        cv2.imwrite(str(temp_path), result)
                        results.append(str(temp_path))
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file.name}: {e}")

        return results

    def process_video(self, video_file, progress=gr.Progress()):
        """å¤„ç†è§†é¢‘æ–‡ä»¶"""
        if video_file is None:
            return None, "è¯·å…ˆä¸Šä¼ è§†é¢‘æ–‡ä»¶"

        try:
            self.load_models_once()

            output_dir = Path("temp_output")
            output_dir.mkdir(exist_ok=True)

            input_path = video_file.name
            output_path = output_dir / f"processed_{int(time.time())}.mp4"

            cap = cv2.VideoCapture(input_path)
            if not cap.isOpened():
                return None, "æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶"

            # è·å–è§†é¢‘å±æ€§
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

            fourcc = cv2.VideoWriter_fourcc(*'H264')
            out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))

            frame_count = 0
            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                processed_frame = self.process_detection(frame, STREAM_CONFIG, use_batch=True)
                out.write(processed_frame)

                frame_count += 1
                if frame_count % 10 == 0:
                    progress_value = frame_count / total_frames
                    progress(progress_value, f"å¤„ç†è¿›åº¦: {frame_count}/{total_frames} å¸§")

            cap.release()
            out.release()
            return str(output_path), f"è§†é¢‘å¤„ç†å®Œæˆï¼å…±å¤„ç† {frame_count} å¸§"

        except Exception as e:
            print(f"âŒ è§†é¢‘å¤„ç†å¤±è´¥: {e}")
            return None, f"è§†é¢‘å¤„ç†å¤±è´¥: {str(e)}"

    def process_camera_frame(self, frame):
        """ä¼˜åŒ–çš„æ‘„åƒå¤´å¸§å¤„ç†"""
        try:
            self.frame_counter += 1

            # è·³å¸§é€»è¾‘
            should_detect = (
                    self.frame_counter % self.frame_skip == 0 or
                    self.last_detection_result is None or
                    self.cache_counter >= self.cache_frames
            )

            if should_detect:
                processed_frame = self.process_detection(frame, STREAM_CONFIG, use_batch=True)
                self.last_detection_result = processed_frame
                self.cache_counter = 0
            else:
                processed_frame = self.last_detection_result if self.last_detection_result is not None else frame
                self.cache_counter += 1

            # æ·»åŠ æ€§èƒ½ä¿¡æ¯
            self._update_fps()
            self._draw_performance_info(processed_frame)

            return processed_frame

        except Exception as e:
            print(f"âŒ æ‘„åƒå¤´å¸§å¤„ç†å¤±è´¥: {e}")
            return frame

    def _update_fps(self):
        """æ›´æ–°FPS"""
        self.fps_counter += 1
        current_time = time.time()
        elapsed = current_time - self.fps_start_time

        if elapsed >= 1.0:
            self.current_fps = self.fps_counter / elapsed
            self.fps_counter = 0
            self.fps_start_time = current_time

    def _draw_performance_info(self, frame):
        """ç»˜åˆ¶æ€§èƒ½ä¿¡æ¯"""
        try:
            info_text = f"FPS: {self.current_fps:.1f} | Skip: 1/{self.frame_skip} | Cache: {self.cache_counter}/{self.cache_frames}"

            # åŠé€æ˜èƒŒæ™¯
            overlay = frame.copy()
            cv2.rectangle(overlay, (10, 10), (400, 40), (0, 0, 0), -1)
            cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)

            # ç»˜åˆ¶æ–‡æœ¬
            cv2.putText(frame, info_text, (15, 30), cv2.FONT_HERSHEY_SIMPLEX,
                        0.5, (0, 255, 0), 1, cv2.LINE_AA)
        except Exception as e:
            print(f"âš ï¸ ç»˜åˆ¶æ€§èƒ½ä¿¡æ¯å¤±è´¥: {e}")

    def get_available_cameras(self):
        """è·å–å¯ç”¨æ‘„åƒå¤´"""
        cameras = []
        for i in range(5):  # å‡å°‘æ£€æµ‹æ•°é‡
            cap = cv2.VideoCapture(i)
            if cap.isOpened():
                ret, _ = cap.read()
                if ret:
                    cameras.append(f"æ‘„åƒå¤´ {i}")
                cap.release()
        return cameras if cameras else ["æ— å¯ç”¨æ‘„åƒå¤´"]

    def start_camera(self, camera_choice, skip_frames=5):
        """å¯åŠ¨æ‘„åƒå¤´"""
        if camera_choice == "æ— å¯ç”¨æ‘„åƒå¤´":
            return "æ²¡æœ‰å¯ç”¨çš„æ‘„åƒå¤´"

        if self.camera_active:
            self.stop_camera()
            time.sleep(0.5)

        try:
            camera_id = int(camera_choice.split()[-1])
            self.frame_skip = max(1, skip_frames)
            self._reset_camera_state()

            self.load_models_once()
            self.camera_active = True

            def camera_worker():
                try:
                    grabber = FrameGrabber(camera_id, queue_size=3)
                    grabber.start()

                    while self.camera_active:
                        ok, frame = grabber.read(timeout=1.0)
                        if not ok:
                            continue

                        processed_frame = self.process_camera_frame(frame)
                        if processed_frame is not None:
                            self.current_frame = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)

                    grabber.stop()
                except Exception as e:
                    print(f"æ‘„åƒå¤´å·¥ä½œçº¿ç¨‹é”™è¯¯: {e}")
                    self.camera_active = False

            self.camera_thread = threading.Thread(target=camera_worker, daemon=True)
            self.camera_thread.start()
            return f"æ‘„åƒå¤´å·²å¯åŠ¨: {camera_choice} (è·³å¸§: 1/{self.frame_skip})"

        except Exception as e:
            self.camera_active = False
            return f"å¯åŠ¨æ‘„åƒå¤´å¤±è´¥: {e}"

    def stop_camera(self):
        """åœæ­¢æ‘„åƒå¤´"""
        self.camera_active = False
        if self.camera_thread:
            self.camera_thread.join(timeout=2.0)
        self._reset_camera_state()
        return "æ‘„åƒå¤´å·²åœæ­¢"

    def _reset_camera_state(self):
        """é‡ç½®æ‘„åƒå¤´çŠ¶æ€"""
        self.current_frame = None
        self.frame_counter = 0
        self.last_detection_result = None
        self.cache_counter = 0
        self.fps_counter = 0
        self.fps_start_time = time.time()

    def get_camera_frame(self):
        """è·å–æ‘„åƒå¤´å¸§"""
        return self.current_frame if self.camera_active else None

    def update_skip_frames(self, skip_frames):
        """æ›´æ–°è·³å¸§è®¾ç½®"""
        self.frame_skip = max(1, skip_frames)
        return f"è·³å¸§è®¾ç½®å·²æ›´æ–°: 1/{self.frame_skip}"


# åˆ›å»ºåº”ç”¨å®ä¾‹
app = OptimizedEmotionApp()


def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    with gr.Blocks(title="æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿ", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# ğŸ­ æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿ")

        with gr.Tabs():
            # å›¾ç‰‡è¯†åˆ«
            with gr.Tab("ğŸ“¸ å›¾ç‰‡è¯†åˆ«"):
                with gr.Row():
                    with gr.Column():
                        image_input = gr.File(
                            label="ä¸Šä¼ å›¾ç‰‡",
                            file_types=["image"],
                            file_count="multiple"
                        )
                        image_btn = gr.Button("å¼€å§‹è¯†åˆ«", variant="primary")

                    with gr.Column():
                        image_output = gr.Gallery(
                            label="è¯†åˆ«ç»“æœ",
                            columns=1,
                            height=None,
                            rows=6,
                            allow_preview=True,
                            preview=True,
                            show_download_button=True,
                            show_share_button=False,
                        )

                image_btn.click(
                    fn=app.process_images_batch,
                    inputs=image_input,
                    outputs=image_output,
                    show_progress=True
                )

            # è§†é¢‘è¯†åˆ«
            with gr.Tab("ğŸ¬ è§†é¢‘è¯†åˆ«"):
                with gr.Row():
                    with gr.Column():
                        video_input = gr.File(
                            label="ä¸Šä¼ è§†é¢‘",
                            file_types=["video"]
                        )
                        video_btn = gr.Button("å¼€å§‹å¤„ç†", variant="primary")

                    with gr.Column():
                        video_status = gr.Textbox(
                            label="å¤„ç†çŠ¶æ€",
                            value="ç­‰å¾…ä¸Šä¼ è§†é¢‘...",
                            interactive=False
                        )
                        video_output = gr.Video(
                            label="å¤„ç†ç»“æœ",
                            height=400
                        )

                video_btn.click(
                    fn=app.process_video,
                    inputs=video_input,
                    outputs=[video_output, video_status],
                    show_progress=True
                )

            # å®æ—¶æ‘„åƒå¤´
            with gr.Tab("ğŸ“¹ å®æ—¶æ‘„åƒå¤´"):
                with gr.Row():
                    with gr.Column(scale=2):
                        camera_dropdown = gr.Dropdown(
                            choices=app.get_available_cameras(),
                            value=app.get_available_cameras()[0],
                            label="é€‰æ‹©æ‘„åƒå¤´"
                        )

                        skip_frames_slider = gr.Slider(
                            minimum=1,
                            maximum=10,
                            value=3,
                            step=1,
                            label="è·³å¸§è®¾ç½®",
                            info="æ•°å€¼è¶Šå¤§æ€§èƒ½è¶Šå¥½"
                        )

                        with gr.Row():
                            start_btn = gr.Button("å¯åŠ¨", variant="primary")
                            stop_btn = gr.Button("åœæ­¢", variant="secondary")

                        update_skip_btn = gr.Button("æ›´æ–°è·³å¸§", variant="secondary")

                        camera_status = gr.Textbox(
                            label="çŠ¶æ€",
                            value="æ‘„åƒå¤´æœªå¯åŠ¨",
                            interactive=False
                        )

                    with gr.Column(scale=2):
                        camera_output = gr.Image(
                            label="å®æ—¶ç”»é¢",
                            height=500
                        )

                # äº‹ä»¶ç»‘å®š
                start_btn.click(
                    fn=lambda cam, skip: app.start_camera(cam, skip),
                    inputs=[camera_dropdown, skip_frames_slider],
                    outputs=camera_status
                )

                stop_btn.click(
                    fn=app.stop_camera,
                    outputs=camera_status
                )

                update_skip_btn.click(
                    fn=app.update_skip_frames,
                    inputs=skip_frames_slider,
                    outputs=camera_status
                )

                # å®šæ—¶å™¨æ›´æ–°æ‘„åƒå¤´ç”»é¢
                camera_timer = gr.Timer(value=0.1)
                camera_timer.tick(
                    fn=app.get_camera_frame,
                    outputs=camera_output
                )

        # ä½¿ç”¨è¯´æ˜
        with gr.Accordion("ä½¿ç”¨è¯´æ˜", open=False):
            gr.Markdown("""
            ### ğŸš€ æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½
            - **è·³å¸§å¤„ç†**: å‡å°‘è®¡ç®—é‡ï¼Œæå‡å®æ—¶æ€§èƒ½
            - **ç»“æœç¼“å­˜**: å¤ç”¨æ£€æµ‹ç»“æœï¼Œé™ä½CPUä½¿ç”¨ç‡
            - **æ‰¹é‡å¤„ç†**: å¤šäººè„¸åœºæ™¯ä¸‹çš„æ‰¹é‡æ¨ç†ä¼˜åŒ–

            ### æ”¯æŒçš„æƒ…æ„Ÿç±»åˆ«
            ğŸ˜  ç”Ÿæ°” | ğŸ¤¢ åŒæ¶ | ğŸ˜¨ ææƒ§ | ğŸ˜Š å¿«ä¹ | ğŸ˜¢ æ‚²ä¼¤ | ğŸ˜ ä¸­æ€§ | ğŸ˜² æƒŠè®¶
            """)

    return interface


if __name__ == "__main__":
    interface = create_interface()
    interface.launch(
        server_name="localhost",
        server_port=7860,
        share=False,
        debug=True
    )